{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LnpeHL6sZoc3","executionInfo":{"status":"ok","timestamp":1756061223100,"user_tz":-360,"elapsed":8769,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"8b86271d-45f2-4b30-9ae6-6fb53970814b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h✅ PyTorch & PyG install attempted.\n"]}],"source":["# ================================\n","# Environment setup (Colab)\n","# ================================\n","!pip -q install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n","!pip -q install torch-geometric torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n","print(\"✅ PyTorch & PyG install attempted.\")"]},{"cell_type":"code","source":["# ================================\n","# Imports & Utilities\n","# ================================\n","import os, random, copy\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.nn import GCNConv, SAGEConv, GraphSAGE\n","from torch_geometric.utils import train_test_split_edges, to_undirected, subgraph\n","from torch_geometric.transforms import RandomLinkSplit\n","from sklearn.metrics import roc_auc_score, average_precision_score\n","\n","# Device & seed\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Device:\", device)\n","\n","def set_seed(seed: int = 42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","set_seed(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLC9XYFDZ8nh","executionInfo":{"status":"ok","timestamp":1756065288903,"user_tz":-360,"elapsed":15,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"84e20731-c268-4bcb-ca63-49d2b7b24053"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}]},{"cell_type":"code","source":["#================================\n","# Load Citeseer + edge split for link prediction (70/10/20)\n","#================================\n","dataset = Planetoid(root='/content/data/Citeseer', name='Citeseer')\n","data = dataset[0]\n","data.edge_index = to_undirected(data.edge_index)  # Ensure undirected\n","# Use RandomLinkSplit for link prediction: splits edges into train/val/test\n","# add_negative_train_samples=True to include negative edges for supervision\n","transform = RandomLinkSplit(num_val=0.1, num_test=0.2, is_undirected=True, add_negative_train_samples=True)\n","train_data, val_data, test_data = transform(data)\n","num_feats = dataset.num_node_features\n","print(train_data)\n","print('Features:', num_feats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W3TaGo3Cl2_J","executionInfo":{"status":"ok","timestamp":1756065291289,"user_tz":-360,"elapsed":58,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"0a6c8d33-089a-4225-e750-d4f1a5ef5700"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data(x=[3327, 3703], edge_index=[2, 6374], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327], edge_label=[6374], edge_label_index=[2, 6374])\n","Features: 3703\n"]}]},{"cell_type":"code","source":["#================================\n","# Config (paper-ish, Citeseer for link prediction)\n","#================================\n","CFG = dict(\n","POS_TRAIN=50, POS_TEST=50,\n","NEG_TRAIN=50, NEG_TEST=50,\n","USE_FT_LAST=True, USE_FT_ALL=True,\n","USE_PR_LAST=True, USE_PR_ALL=True,\n","USE_DISTILL=True,\n","DISTILL_STEPS=250,   # paper ~1000; adjust if GPU/time allows\n","FP_P=64,\n","FP_NODES=32,\n","FP_SAMPLE_M=32,  # Number of node pairs to sample per fingerprint graph\n","FP_EDGE_INIT_P=0.05,\n","FP_EDGE_TOPK=96,\n","EDGE_LOGIT_STEP=2.5,\n","OUTER_ITERS=20,\n","FP_STEPS=5,\n","V_STEPS=10,\n","LR_TARGET=0.005,\n","WD_TARGET=5e-4,\n","LR_V=1e-3,\n","LR_X=1e-3,\n","SEED=1,\n",")\n","print(CFG)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g6jgFX1smR-u","executionInfo":{"status":"ok","timestamp":1756065293378,"user_tz":-360,"elapsed":15,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"60ff200f-d578-4cb3-a080-c9e5ac76266c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'POS_TRAIN': 50, 'POS_TEST': 50, 'NEG_TRAIN': 50, 'NEG_TEST': 50, 'USE_FT_LAST': True, 'USE_FT_ALL': True, 'USE_PR_LAST': True, 'USE_PR_ALL': True, 'USE_DISTILL': True, 'DISTILL_STEPS': 250, 'FP_P': 64, 'FP_NODES': 32, 'FP_SAMPLE_M': 32, 'FP_EDGE_INIT_P': 0.05, 'FP_EDGE_TOPK': 96, 'EDGE_LOGIT_STEP': 2.5, 'OUTER_ITERS': 20, 'FP_STEPS': 5, 'V_STEPS': 10, 'LR_TARGET': 0.005, 'WD_TARGET': 0.0005, 'LR_V': 0.001, 'LR_X': 0.001, 'SEED': 1}\n"]}]},{"cell_type":"code","source":["#================================\n","# Define 3-layer GNN models (output embeddings for link prediction)\n","#================================\n","class GCN(nn.Module):\n","    def __init__(self, in_channels, hidden, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = GCNConv(in_channels, hidden)\n","        self.conv2 = GCNConv(hidden, hidden)\n","        self.conv3 = GCNConv(hidden, hidden)  # Output embeddings of size hidden\n","        self.dropout = dropout\n","\n","    def forward(self, x, edge_index):\n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv3(x, edge_index)\n","        return x\n","\n","class GraphSAGE(nn.Module):\n","    def __init__(self, in_channels, hidden, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden)\n","        self.conv2 = SAGEConv(hidden, hidden)\n","        self.conv3 = SAGEConv(hidden, hidden)  # Output embeddings of size hidden\n","        self.dropout = dropout\n","\n","    def forward(self, x, edge_index):\n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv3(x, edge_index)\n","        return x"],"metadata":{"id":"SU91jMIGmaR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#================================\n","# Train helpers for link prediction\n","#================================\n","@torch.no_grad()\n","def evaluate(model, loader_data):\n","    model.eval()\n","    h = model(loader_data.x.to(device), loader_data.edge_index.to(device))\n","    edge_label_index = loader_data.edge_label_index.to(device)\n","    edge_label = loader_data.edge_label.to(device).float()\n","    h_src = h[edge_label_index[0]]\n","    h_dst = h[edge_label_index[1]]\n","    scores = torch.sigmoid((h_src * h_dst).sum(dim=-1))\n","    pred = (scores > 0.5).float()\n","    acc = (pred == edge_label).float().mean().item()\n","    return acc\n","def train_link_pred(model, train_data, val_data, epochs=200, lr=0.005, wd=5e-4):\n","    model = model.to(device)\n","    train_data = train_data.to(device)\n","    val_data = val_data.to(device)\n","    opt = Adam(model.parameters(), lr=lr, weight_decay=wd)\n","    best = {'val': 0.0, 'state': None}\n","    for ep in range(epochs):\n","        model.train(); opt.zero_grad()\n","        h = model(train_data.x, train_data.edge_index)\n","        edge_label_index = train_data.edge_label_index\n","        edge_label = train_data.edge_label.float()\n","        h_src = h[edge_label_index[0]]\n","        h_dst = h[edge_label_index[1]]\n","        scores = torch.sigmoid((h_src * h_dst).sum(dim=-1))\n","        loss = F.binary_cross_entropy(scores, edge_label)\n","        loss.backward(); opt.step()\n","        va = evaluate(model, val_data)\n","        if va > best['val']:\n","            best['val'] = va; best['state'] = copy.deepcopy(model.state_dict())\n","        if ep % 20 == 0:\n","            print(f\"Epoch {ep:03d} | loss {loss.item():.4f} | val {va:.3f}\")\n","    if best['state'] is not None:\n","        model.load_state_dict(best['state'])\n","    te = evaluate(model, test_data.to(device))\n","    print(f\"✅ Final (best-val) | val {best['val']:.3f} | test {te:.3f}\")\n","    return model"],"metadata":{"id":"5Wo4qcH8mpK1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train target model f (GCN for link prediction)\n","#================================\n","set_seed(CFG[\"SEED\"])\n","model_f = GCN(num_feats, hidden=16, dropout=0.5)\n","model_f = train_link_pred(model_f, train_data, val_data, epochs=200, lr=CFG[\"LR_TARGET\"], wd=CFG[\"WD_TARGET\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHdu0ZnBm0xF","executionInfo":{"status":"ok","timestamp":1756065303567,"user_tz":-360,"elapsed":1461,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"1ae6a9d0-83b0-4b4b-b01c-5523211207ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 000 | loss 0.6900 | val 0.504\n","Epoch 020 | loss 0.4350 | val 0.724\n","Epoch 040 | loss 0.3384 | val 0.703\n","Epoch 060 | loss 0.2734 | val 0.686\n","Epoch 080 | loss 0.2241 | val 0.699\n","Epoch 100 | loss 0.2020 | val 0.698\n","Epoch 120 | loss 0.1766 | val 0.698\n","Epoch 140 | loss 0.1746 | val 0.691\n","Epoch 160 | loss 0.1564 | val 0.688\n","Epoch 180 | loss 0.1614 | val 0.709\n","✅ Final (best-val) | val 0.726 | test 0.698\n"]}]},{"cell_type":"code","source":["#=========================================\n","# Build suspect models (F+ and F−)  [BUDGETED, adapted for link prediction]\n","#=========================================\n","@torch.no_grad()\n","def reset_module(m):\n","    for layer in m.modules():\n","        if hasattr(layer, 'reset_parameters'):\n","            layer.reset_parameters()\n","\n","def ft_model(base_model, train_data, last_only=True, epochs=10, lr=0.005, seed=123):\n","    set_seed(seed)\n","    m = copy.deepcopy(base_model).to(device)\n","    for p in m.parameters(): p.requires_grad_(not last_only)\n","    for p in m.conv3.parameters(): p.requires_grad_(True)  # last layer fine-tune\n","    opt = Adam(filter(lambda p: p.requires_grad, m.parameters()), lr=lr)\n","    for _ in range(epochs):\n","        m.train(); opt.zero_grad()\n","        h = m(train_data.x, train_data.edge_index)\n","        edge_label_index = train_data.edge_label_index\n","        edge_label = train_data.edge_label.float()\n","        h_src = h[edge_label_index[0]]\n","        h_dst = h[edge_label_index[1]]\n","        scores = torch.sigmoid((h_src * h_dst).sum(dim=-1))\n","        loss = F.binary_cross_entropy(scores, edge_label)\n","        loss.backward(); opt.step()\n","    return m.eval()\n","\n","def pr_model(base_model, train_data, last_only=True, epochs=10, lr=0.005, seed=456):\n","    set_seed(seed)\n","    m = copy.deepcopy(base_model).to(device)\n","    if last_only: reset_module(m.conv3)\n","    else:         reset_module(m)\n","    opt = Adam(m.parameters(), lr=lr)\n","    for _ in range(epochs):\n","        m.train(); opt.zero_grad()\n","        h = m(train_data.x, train_data.edge_index)\n","        edge_label_index = train_data.edge_label_index\n","        edge_label = train_data.edge_label.float()\n","        h_src = h[edge_label_index[0]]\n","        h_dst = h[edge_label_index[1]]\n","        scores = torch.sigmoid((h_src * h_dst).sum(dim=-1))\n","        loss = F.binary_cross_entropy(scores, edge_label)\n","        loss.backward(); opt.step()\n","    return m.eval()\n","\n","def make_student(arch='GCN', hidden=16):\n","    return (GCN(num_feats, hidden, dropout=0.5).to(device)\n","            if arch=='GCN' else\n","            GraphSAGE(num_feats, hidden, dropout=0.5).to(device))\n","\n","def random_subgraph_idx(n, keep_ratio=0.6, seed=7):\n","    g = torch.Generator().manual_seed(seed)\n","    keep = int(n*keep_ratio)\n","    return torch.randperm(n, generator=g)[:keep]\n","\n","def distill_from_teacher(teacher, data, arch='GCN', steps=250, lr=0.01, seed=777):\n","    set_seed(seed)\n","    student = make_student(arch, hidden=16)\n","    opt = Adam(student.parameters(), lr=lr)\n","    mse = nn.MSELoss()\n","    x_all = data.x.to(device); ei_all = data.edge_index.to(device)\n","    for t in range(steps):\n","        keep_ratio = float(torch.empty(1).uniform_(0.5, 0.8))\n","        idx = random_subgraph_idx(data.num_nodes, keep_ratio=keep_ratio, seed=seed+t).to(device)\n","        ei_sub, _ = subgraph(idx, ei_all, relabel_nodes=True)\n","        x_sub = x_all[idx]\n","        with torch.no_grad():\n","            h_t = teacher(x_sub, ei_sub)\n","        student.train(); opt.zero_grad()\n","        h_s = student(x_sub, ei_sub)\n","        loss = mse(h_s, h_t)\n","        loss.backward(); opt.step()\n","    return student.eval()\n","\n","#---- budget splitter ----\n","def _distribute_budget(total, keys):\n","    if not keys: return {}\n","    base = total // len(keys); rem = total - base*len(keys)\n","    out = {k: base for k in keys}\n","    for k in keys[:rem]: out[k] += 1\n","    return out\n","\n","#===== Positives (F+) =====\n","F_pos_all = []\n","pos_total = CFG[\"POS_TRAIN\"] + CFG[\"POS_TEST\"]\n","pos_keys = []\n","if CFG[\"USE_FT_LAST\"]: pos_keys.append(\"FT_LAST\")\n","if CFG[\"USE_FT_ALL\"]:  pos_keys.append(\"FT_ALL\")\n","if CFG[\"USE_PR_LAST\"]: pos_keys.append(\"PR_LAST\")\n","if CFG[\"USE_PR_ALL\"]:  pos_keys.append(\"PR_ALL\")\n","if CFG[\"USE_DISTILL\"]: pos_keys.append(\"DISTILL\")\n","pos_budget = _distribute_budget(pos_total, pos_keys)\n","seed_base = 10\n","for key in pos_keys:\n","    cnt = pos_budget[key]\n","    if key == \"FT_LAST\":\n","        for s in range(seed_base, seed_base+cnt):\n","            F_pos_all.append(ft_model(model_f, train_data, last_only=True,  epochs=10, seed=s))\n","        seed_base += cnt\n","    elif key == \"FT_ALL\":\n","        for s in range(seed_base, seed_base+cnt):\n","            F_pos_all.append(ft_model(model_f, train_data, last_only=False, epochs=10, seed=s))\n","        seed_base += cnt\n","    elif key == \"PR_LAST\":\n","        for s in range(seed_base, seed_base+cnt):\n","            F_pos_all.append(pr_model(model_f, train_data, last_only=True,  epochs=10, seed=s))\n","        seed_base += cnt\n","    elif key == \"PR_ALL\":\n","        for s in range(seed_base, seed_base+cnt):\n","            F_pos_all.append(pr_model(model_f, train_data, last_only=False, epochs=10, seed=s))\n","        seed_base += cnt\n","    elif key == \"DISTILL\":\n","        arches = (['GCN'] * (cnt//2) + ['SAGE'] * (cnt - cnt//2))\n","        for i, arch in enumerate(arches, 400):\n","            F_pos_all.append(distill_from_teacher(model_f, data, arch=arch,\n","                                                steps=CFG[\"DISTILL_STEPS\"], seed=1000+i))\n","        seed_base += cnt\n","\n","assert len(F_pos_all) == pos_total, f\"Expected {pos_total} positives, got {len(F_pos_all)}\"\n","\n","#===== Negatives (F−) =====\n","F_neg_all = []\n","neg_total = CFG[\"NEG_TRAIN\"] + CFG[\"NEG_TEST\"]\n","neg_keys = [\"GCN\", \"SAGE\"]\n","neg_budget = _distribute_budget(neg_total, neg_keys)\n","seed_base = 500\n","for s in range(seed_base, seed_base+neg_budget[\"GCN\"]):\n","    set_seed(s)\n","    m = GCN(num_feats, 16, dropout=0.5)\n","    m = train_link_pred(m, train_data, val_data, epochs=120, lr=CFG[\"LR_TARGET\"], wd=CFG[\"WD_TARGET\"])\n","    F_neg_all.append(m.eval())\n","seed_base += neg_budget[\"GCN\"]\n","for s in range(seed_base, seed_base+neg_budget[\"SAGE\"]):\n","    set_seed(s)\n","    m = GraphSAGE(num_feats, 32, dropout=0.5)\n","    m = train_link_pred(m, train_data, val_data, epochs=120, lr=CFG[\"LR_TARGET\"], wd=CFG[\"WD_TARGET\"])\n","    F_neg_all.append(m.eval())\n","\n","#===== Train/Test split =====\n","def split_pool(pool, n_train, n_test, seed=999):\n","    set_seed(seed)\n","    idx = torch.randperm(len(pool)).tolist()\n","    train = [pool[i] for i in idx[:n_train]]\n","    test  = [pool[i] for i in idx[n_train:n_train+n_test]]\n","    return train, test\n","\n","F_pos_tr, F_pos_te = split_pool(F_pos_all, CFG[\"POS_TRAIN\"], CFG[\"POS_TEST\"])\n","F_neg_tr, F_neg_te = split_pool(F_neg_all, CFG[\"NEG_TRAIN\"], CFG[\"NEG_TEST\"])\n","print(f\"F+ train/test: {len(F_pos_tr)}/{len(F_pos_te)} | F- train/test: {len(F_neg_tr)}/{len(F_neg_te)} (totals match budgets)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DAccFCoUm7eG","executionInfo":{"status":"ok","timestamp":1756065441858,"user_tz":-360,"elapsed":134980,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"520e9b88-84bd-4303-c206-3fecf620bd44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 000 | loss 0.6888 | val 0.499\n","Epoch 020 | loss 0.4150 | val 0.727\n","Epoch 040 | loss 0.3185 | val 0.711\n","Epoch 060 | loss 0.2563 | val 0.692\n","Epoch 080 | loss 0.2080 | val 0.685\n","Epoch 100 | loss 0.1894 | val 0.679\n","✅ Final (best-val) | val 0.735 | test 0.723\n","Epoch 000 | loss 0.6907 | val 0.500\n","Epoch 020 | loss 0.4350 | val 0.713\n","Epoch 040 | loss 0.3433 | val 0.701\n","Epoch 060 | loss 0.2987 | val 0.681\n","Epoch 080 | loss 0.2513 | val 0.685\n","Epoch 100 | loss 0.2204 | val 0.696\n","✅ Final (best-val) | val 0.744 | test 0.716\n","Epoch 000 | loss 0.6859 | val 0.499\n","Epoch 020 | loss 0.4111 | val 0.699\n","Epoch 040 | loss 0.3244 | val 0.685\n","Epoch 060 | loss 0.2737 | val 0.682\n","Epoch 080 | loss 0.2352 | val 0.671\n","Epoch 100 | loss 0.2229 | val 0.669\n","✅ Final (best-val) | val 0.736 | test 0.702\n","Epoch 000 | loss 0.6893 | val 0.512\n","Epoch 020 | loss 0.4129 | val 0.725\n","Epoch 040 | loss 0.3199 | val 0.720\n","Epoch 060 | loss 0.2565 | val 0.682\n","Epoch 080 | loss 0.2259 | val 0.707\n","Epoch 100 | loss 0.1866 | val 0.708\n","✅ Final (best-val) | val 0.736 | test 0.707\n","Epoch 000 | loss 0.6888 | val 0.500\n","Epoch 020 | loss 0.4278 | val 0.709\n","Epoch 040 | loss 0.3259 | val 0.709\n","Epoch 060 | loss 0.2672 | val 0.703\n","Epoch 080 | loss 0.2294 | val 0.690\n","Epoch 100 | loss 0.2004 | val 0.693\n","✅ Final (best-val) | val 0.729 | test 0.696\n","Epoch 000 | loss 0.6896 | val 0.500\n","Epoch 020 | loss 0.4451 | val 0.705\n","Epoch 040 | loss 0.3595 | val 0.691\n","Epoch 060 | loss 0.3023 | val 0.682\n","Epoch 080 | loss 0.2532 | val 0.678\n","Epoch 100 | loss 0.2148 | val 0.679\n","✅ Final (best-val) | val 0.709 | test 0.713\n","Epoch 000 | loss 0.6897 | val 0.501\n","Epoch 020 | loss 0.4301 | val 0.744\n","Epoch 040 | loss 0.3369 | val 0.710\n","Epoch 060 | loss 0.2839 | val 0.701\n","Epoch 080 | loss 0.2344 | val 0.708\n","Epoch 100 | loss 0.2197 | val 0.700\n","✅ Final (best-val) | val 0.744 | test 0.726\n","Epoch 000 | loss 0.6895 | val 0.500\n","Epoch 020 | loss 0.4523 | val 0.712\n","Epoch 040 | loss 0.3655 | val 0.699\n","Epoch 060 | loss 0.3057 | val 0.684\n","Epoch 080 | loss 0.2529 | val 0.671\n","Epoch 100 | loss 0.2307 | val 0.677\n","✅ Final (best-val) | val 0.722 | test 0.681\n","Epoch 000 | loss 0.6879 | val 0.500\n","Epoch 020 | loss 0.4571 | val 0.699\n","Epoch 040 | loss 0.3601 | val 0.722\n","Epoch 060 | loss 0.2976 | val 0.716\n","Epoch 080 | loss 0.2452 | val 0.702\n","Epoch 100 | loss 0.2143 | val 0.703\n","✅ Final (best-val) | val 0.727 | test 0.697\n","Epoch 000 | loss 0.6879 | val 0.500\n","Epoch 020 | loss 0.4199 | val 0.715\n","Epoch 040 | loss 0.3266 | val 0.690\n","Epoch 060 | loss 0.2738 | val 0.691\n","Epoch 080 | loss 0.2388 | val 0.681\n","Epoch 100 | loss 0.2097 | val 0.677\n","✅ Final (best-val) | val 0.729 | test 0.697\n","Epoch 000 | loss 0.6901 | val 0.505\n","Epoch 020 | loss 0.4352 | val 0.730\n","Epoch 040 | loss 0.3381 | val 0.729\n","Epoch 060 | loss 0.2781 | val 0.722\n","Epoch 080 | loss 0.2326 | val 0.716\n","Epoch 100 | loss 0.1954 | val 0.726\n","✅ Final (best-val) | val 0.743 | test 0.726\n","Epoch 000 | loss 0.6905 | val 0.507\n","Epoch 020 | loss 0.4337 | val 0.744\n","Epoch 040 | loss 0.3310 | val 0.689\n","Epoch 060 | loss 0.2741 | val 0.705\n","Epoch 080 | loss 0.2361 | val 0.703\n","Epoch 100 | loss 0.1881 | val 0.691\n","✅ Final (best-val) | val 0.747 | test 0.728\n","Epoch 000 | loss 0.6899 | val 0.500\n","Epoch 020 | loss 0.4355 | val 0.705\n","Epoch 040 | loss 0.3359 | val 0.713\n","Epoch 060 | loss 0.2660 | val 0.702\n","Epoch 080 | loss 0.2136 | val 0.699\n","Epoch 100 | loss 0.1785 | val 0.695\n","✅ Final (best-val) | val 0.720 | test 0.705\n","Epoch 000 | loss 0.6887 | val 0.501\n","Epoch 020 | loss 0.4175 | val 0.696\n","Epoch 040 | loss 0.3295 | val 0.691\n","Epoch 060 | loss 0.2766 | val 0.688\n","Epoch 080 | loss 0.2364 | val 0.687\n","Epoch 100 | loss 0.1935 | val 0.682\n","✅ Final (best-val) | val 0.707 | test 0.703\n","Epoch 000 | loss 0.6908 | val 0.500\n","Epoch 020 | loss 0.4446 | val 0.740\n","Epoch 040 | loss 0.3415 | val 0.741\n","Epoch 060 | loss 0.2833 | val 0.723\n","Epoch 080 | loss 0.2346 | val 0.715\n","Epoch 100 | loss 0.1975 | val 0.710\n","✅ Final (best-val) | val 0.760 | test 0.719\n","Epoch 000 | loss 0.6903 | val 0.507\n","Epoch 020 | loss 0.4399 | val 0.712\n","Epoch 040 | loss 0.3467 | val 0.690\n","Epoch 060 | loss 0.2747 | val 0.651\n","Epoch 080 | loss 0.2322 | val 0.645\n","Epoch 100 | loss 0.2038 | val 0.646\n","✅ Final (best-val) | val 0.723 | test 0.710\n","Epoch 000 | loss 0.6890 | val 0.500\n","Epoch 020 | loss 0.4407 | val 0.737\n","Epoch 040 | loss 0.3467 | val 0.712\n","Epoch 060 | loss 0.2734 | val 0.709\n","Epoch 080 | loss 0.2381 | val 0.712\n","Epoch 100 | loss 0.2097 | val 0.699\n","✅ Final (best-val) | val 0.748 | test 0.727\n","Epoch 000 | loss 0.6893 | val 0.500\n","Epoch 020 | loss 0.4385 | val 0.720\n","Epoch 040 | loss 0.3332 | val 0.714\n","Epoch 060 | loss 0.2701 | val 0.705\n","Epoch 080 | loss 0.2256 | val 0.702\n","Epoch 100 | loss 0.1956 | val 0.711\n","✅ Final (best-val) | val 0.734 | test 0.735\n","Epoch 000 | loss 0.6898 | val 0.500\n","Epoch 020 | loss 0.4541 | val 0.697\n","Epoch 040 | loss 0.3604 | val 0.681\n","Epoch 060 | loss 0.2854 | val 0.682\n","Epoch 080 | loss 0.2464 | val 0.695\n","Epoch 100 | loss 0.2074 | val 0.687\n","✅ Final (best-val) | val 0.703 | test 0.695\n","Epoch 000 | loss 0.6890 | val 0.500\n","Epoch 020 | loss 0.4428 | val 0.733\n","Epoch 040 | loss 0.3520 | val 0.724\n","Epoch 060 | loss 0.2908 | val 0.710\n","Epoch 080 | loss 0.2478 | val 0.715\n","Epoch 100 | loss 0.2195 | val 0.695\n","✅ Final (best-val) | val 0.733 | test 0.724\n","Epoch 000 | loss 0.6881 | val 0.500\n","Epoch 020 | loss 0.4310 | val 0.721\n","Epoch 040 | loss 0.3334 | val 0.693\n","Epoch 060 | loss 0.2679 | val 0.699\n","Epoch 080 | loss 0.2247 | val 0.678\n","Epoch 100 | loss 0.2098 | val 0.682\n","✅ Final (best-val) | val 0.725 | test 0.700\n","Epoch 000 | loss 0.6886 | val 0.500\n","Epoch 020 | loss 0.4249 | val 0.720\n","Epoch 040 | loss 0.3282 | val 0.724\n","Epoch 060 | loss 0.2595 | val 0.714\n","Epoch 080 | loss 0.2286 | val 0.708\n","Epoch 100 | loss 0.1993 | val 0.712\n","✅ Final (best-val) | val 0.732 | test 0.699\n","Epoch 000 | loss 0.6881 | val 0.500\n","Epoch 020 | loss 0.4362 | val 0.702\n","Epoch 040 | loss 0.3415 | val 0.720\n","Epoch 060 | loss 0.2676 | val 0.710\n","Epoch 080 | loss 0.2109 | val 0.704\n","Epoch 100 | loss 0.1688 | val 0.695\n","✅ Final (best-val) | val 0.723 | test 0.712\n","Epoch 000 | loss 0.6880 | val 0.500\n","Epoch 020 | loss 0.4415 | val 0.716\n","Epoch 040 | loss 0.3351 | val 0.693\n","Epoch 060 | loss 0.2644 | val 0.671\n","Epoch 080 | loss 0.2169 | val 0.680\n","Epoch 100 | loss 0.1892 | val 0.686\n","✅ Final (best-val) | val 0.730 | test 0.701\n","Epoch 000 | loss 0.6897 | val 0.501\n","Epoch 020 | loss 0.4312 | val 0.736\n","Epoch 040 | loss 0.3454 | val 0.714\n","Epoch 060 | loss 0.2936 | val 0.715\n","Epoch 080 | loss 0.2398 | val 0.719\n","Epoch 100 | loss 0.2128 | val 0.715\n","✅ Final (best-val) | val 0.738 | test 0.697\n","Epoch 000 | loss 0.6898 | val 0.501\n","Epoch 020 | loss 0.4266 | val 0.703\n","Epoch 040 | loss 0.3392 | val 0.678\n","Epoch 060 | loss 0.2682 | val 0.675\n","Epoch 080 | loss 0.2265 | val 0.662\n","Epoch 100 | loss 0.2051 | val 0.655\n","✅ Final (best-val) | val 0.703 | test 0.701\n","Epoch 000 | loss 0.6892 | val 0.500\n","Epoch 020 | loss 0.4401 | val 0.721\n","Epoch 040 | loss 0.3382 | val 0.702\n","Epoch 060 | loss 0.2821 | val 0.682\n","Epoch 080 | loss 0.2268 | val 0.677\n","Epoch 100 | loss 0.2119 | val 0.690\n","✅ Final (best-val) | val 0.724 | test 0.705\n","Epoch 000 | loss 0.6873 | val 0.500\n","Epoch 020 | loss 0.4492 | val 0.709\n","Epoch 040 | loss 0.3742 | val 0.713\n","Epoch 060 | loss 0.3108 | val 0.700\n","Epoch 080 | loss 0.2583 | val 0.701\n","Epoch 100 | loss 0.2316 | val 0.702\n","✅ Final (best-val) | val 0.724 | test 0.703\n","Epoch 000 | loss 0.6908 | val 0.501\n","Epoch 020 | loss 0.4578 | val 0.721\n","Epoch 040 | loss 0.3682 | val 0.704\n","Epoch 060 | loss 0.2841 | val 0.698\n","Epoch 080 | loss 0.2451 | val 0.708\n","Epoch 100 | loss 0.2076 | val 0.699\n","✅ Final (best-val) | val 0.729 | test 0.699\n","Epoch 000 | loss 0.6894 | val 0.501\n","Epoch 020 | loss 0.4364 | val 0.713\n","Epoch 040 | loss 0.3420 | val 0.679\n","Epoch 060 | loss 0.2829 | val 0.674\n","Epoch 080 | loss 0.2441 | val 0.675\n","Epoch 100 | loss 0.2324 | val 0.675\n","✅ Final (best-val) | val 0.729 | test 0.717\n","Epoch 000 | loss 0.6907 | val 0.500\n","Epoch 020 | loss 0.4388 | val 0.698\n","Epoch 040 | loss 0.3466 | val 0.698\n","Epoch 060 | loss 0.2853 | val 0.699\n","Epoch 080 | loss 0.2496 | val 0.695\n","Epoch 100 | loss 0.2537 | val 0.690\n","✅ Final (best-val) | val 0.712 | test 0.708\n","Epoch 000 | loss 0.6898 | val 0.510\n","Epoch 020 | loss 0.4342 | val 0.737\n","Epoch 040 | loss 0.3485 | val 0.703\n","Epoch 060 | loss 0.2918 | val 0.714\n","Epoch 080 | loss 0.2455 | val 0.703\n","Epoch 100 | loss 0.2038 | val 0.700\n","✅ Final (best-val) | val 0.741 | test 0.729\n","Epoch 000 | loss 0.6901 | val 0.501\n","Epoch 020 | loss 0.4201 | val 0.714\n","Epoch 040 | loss 0.3163 | val 0.714\n","Epoch 060 | loss 0.2572 | val 0.697\n","Epoch 080 | loss 0.2174 | val 0.695\n","Epoch 100 | loss 0.1691 | val 0.692\n","✅ Final (best-val) | val 0.723 | test 0.703\n","Epoch 000 | loss 0.6883 | val 0.502\n","Epoch 020 | loss 0.4507 | val 0.731\n","Epoch 040 | loss 0.3628 | val 0.715\n","Epoch 060 | loss 0.2937 | val 0.695\n","Epoch 080 | loss 0.2509 | val 0.691\n","Epoch 100 | loss 0.2220 | val 0.697\n","✅ Final (best-val) | val 0.734 | test 0.708\n","Epoch 000 | loss 0.6903 | val 0.500\n","Epoch 020 | loss 0.4498 | val 0.721\n","Epoch 040 | loss 0.3533 | val 0.708\n","Epoch 060 | loss 0.2868 | val 0.697\n","Epoch 080 | loss 0.2370 | val 0.685\n","Epoch 100 | loss 0.2120 | val 0.693\n","✅ Final (best-val) | val 0.738 | test 0.706\n","Epoch 000 | loss 0.6879 | val 0.503\n","Epoch 020 | loss 0.4267 | val 0.682\n","Epoch 040 | loss 0.3454 | val 0.671\n","Epoch 060 | loss 0.2871 | val 0.669\n","Epoch 080 | loss 0.2474 | val 0.658\n","Epoch 100 | loss 0.2282 | val 0.662\n","✅ Final (best-val) | val 0.704 | test 0.702\n","Epoch 000 | loss 0.6877 | val 0.501\n","Epoch 020 | loss 0.4014 | val 0.734\n","Epoch 040 | loss 0.3054 | val 0.729\n","Epoch 060 | loss 0.2472 | val 0.725\n","Epoch 080 | loss 0.2195 | val 0.713\n","Epoch 100 | loss 0.1955 | val 0.710\n","✅ Final (best-val) | val 0.742 | test 0.702\n","Epoch 000 | loss 0.6901 | val 0.501\n","Epoch 020 | loss 0.4213 | val 0.731\n","Epoch 040 | loss 0.3203 | val 0.732\n","Epoch 060 | loss 0.2555 | val 0.715\n","Epoch 080 | loss 0.2058 | val 0.699\n","Epoch 100 | loss 0.1859 | val 0.710\n","✅ Final (best-val) | val 0.742 | test 0.727\n","Epoch 000 | loss 0.6891 | val 0.500\n","Epoch 020 | loss 0.4559 | val 0.718\n","Epoch 040 | loss 0.3574 | val 0.695\n","Epoch 060 | loss 0.2855 | val 0.691\n","Epoch 080 | loss 0.2495 | val 0.675\n","Epoch 100 | loss 0.2091 | val 0.673\n","✅ Final (best-val) | val 0.725 | test 0.719\n","Epoch 000 | loss 0.6912 | val 0.500\n","Epoch 020 | loss 0.4425 | val 0.705\n","Epoch 040 | loss 0.3526 | val 0.712\n","Epoch 060 | loss 0.2935 | val 0.707\n","Epoch 080 | loss 0.2553 | val 0.709\n","Epoch 100 | loss 0.2063 | val 0.713\n","✅ Final (best-val) | val 0.720 | test 0.692\n","Epoch 000 | loss 0.6876 | val 0.500\n","Epoch 020 | loss 0.4488 | val 0.714\n","Epoch 040 | loss 0.3594 | val 0.707\n","Epoch 060 | loss 0.2919 | val 0.708\n","Epoch 080 | loss 0.2430 | val 0.698\n","Epoch 100 | loss 0.2109 | val 0.690\n","✅ Final (best-val) | val 0.721 | test 0.709\n","Epoch 000 | loss 0.6884 | val 0.501\n","Epoch 020 | loss 0.4501 | val 0.712\n","Epoch 040 | loss 0.3378 | val 0.696\n","Epoch 060 | loss 0.2869 | val 0.695\n","Epoch 080 | loss 0.2217 | val 0.679\n","Epoch 100 | loss 0.2135 | val 0.678\n","✅ Final (best-val) | val 0.718 | test 0.703\n","Epoch 000 | loss 0.6887 | val 0.500\n","Epoch 020 | loss 0.4375 | val 0.725\n","Epoch 040 | loss 0.3400 | val 0.700\n","Epoch 060 | loss 0.2739 | val 0.682\n","Epoch 080 | loss 0.2332 | val 0.684\n","Epoch 100 | loss 0.2057 | val 0.689\n","✅ Final (best-val) | val 0.732 | test 0.719\n","Epoch 000 | loss 0.6881 | val 0.500\n","Epoch 020 | loss 0.4311 | val 0.707\n","Epoch 040 | loss 0.3325 | val 0.695\n","Epoch 060 | loss 0.2739 | val 0.670\n","Epoch 080 | loss 0.2346 | val 0.667\n","Epoch 100 | loss 0.2049 | val 0.673\n","✅ Final (best-val) | val 0.718 | test 0.693\n","Epoch 000 | loss 0.6897 | val 0.500\n","Epoch 020 | loss 0.4239 | val 0.723\n","Epoch 040 | loss 0.3382 | val 0.696\n","Epoch 060 | loss 0.2791 | val 0.690\n","Epoch 080 | loss 0.2384 | val 0.690\n","Epoch 100 | loss 0.2150 | val 0.682\n","✅ Final (best-val) | val 0.730 | test 0.695\n","Epoch 000 | loss 0.6904 | val 0.508\n","Epoch 020 | loss 0.4364 | val 0.721\n","Epoch 040 | loss 0.3422 | val 0.730\n","Epoch 060 | loss 0.2902 | val 0.723\n","Epoch 080 | loss 0.2502 | val 0.710\n","Epoch 100 | loss 0.2161 | val 0.710\n","✅ Final (best-val) | val 0.737 | test 0.726\n","Epoch 000 | loss 0.6871 | val 0.501\n","Epoch 020 | loss 0.4193 | val 0.735\n","Epoch 040 | loss 0.3220 | val 0.714\n","Epoch 060 | loss 0.2608 | val 0.700\n","Epoch 080 | loss 0.2164 | val 0.703\n","Epoch 100 | loss 0.1939 | val 0.704\n","✅ Final (best-val) | val 0.735 | test 0.714\n","Epoch 000 | loss 0.6890 | val 0.500\n","Epoch 020 | loss 0.4124 | val 0.721\n","Epoch 040 | loss 0.3129 | val 0.691\n","Epoch 060 | loss 0.2579 | val 0.684\n","Epoch 080 | loss 0.2164 | val 0.674\n","Epoch 100 | loss 0.1828 | val 0.669\n","✅ Final (best-val) | val 0.724 | test 0.711\n","Epoch 000 | loss 0.6891 | val 0.500\n","Epoch 020 | loss 0.4415 | val 0.718\n","Epoch 040 | loss 0.3527 | val 0.721\n","Epoch 060 | loss 0.2825 | val 0.718\n","Epoch 080 | loss 0.2422 | val 0.701\n","Epoch 100 | loss 0.2070 | val 0.714\n","✅ Final (best-val) | val 0.731 | test 0.697\n","Epoch 000 | loss 0.6884 | val 0.500\n","Epoch 020 | loss 0.4242 | val 0.716\n","Epoch 040 | loss 0.3274 | val 0.692\n","Epoch 060 | loss 0.2712 | val 0.681\n","Epoch 080 | loss 0.2250 | val 0.682\n","Epoch 100 | loss 0.1937 | val 0.685\n","✅ Final (best-val) | val 0.727 | test 0.712\n","Epoch 000 | loss 0.7196 | val 0.500\n","Epoch 020 | loss 0.4016 | val 0.679\n","Epoch 040 | loss 0.2474 | val 0.677\n","Epoch 060 | loss 0.2023 | val 0.659\n","Epoch 080 | loss 0.1378 | val 0.667\n","Epoch 100 | loss 0.1070 | val 0.676\n","✅ Final (best-val) | val 0.685 | test 0.676\n","Epoch 000 | loss 0.7273 | val 0.500\n","Epoch 020 | loss 0.4024 | val 0.677\n","Epoch 040 | loss 0.2823 | val 0.664\n","Epoch 060 | loss 0.1974 | val 0.679\n","Epoch 080 | loss 0.1227 | val 0.680\n","Epoch 100 | loss 0.0977 | val 0.686\n","✅ Final (best-val) | val 0.690 | test 0.694\n","Epoch 000 | loss 0.7306 | val 0.500\n","Epoch 020 | loss 0.3698 | val 0.646\n","Epoch 040 | loss 0.2369 | val 0.656\n","Epoch 060 | loss 0.1765 | val 0.681\n","Epoch 080 | loss 0.1098 | val 0.680\n","Epoch 100 | loss 0.0927 | val 0.679\n","✅ Final (best-val) | val 0.692 | test 0.696\n","Epoch 000 | loss 0.7063 | val 0.500\n","Epoch 020 | loss 0.3784 | val 0.678\n","Epoch 040 | loss 0.2353 | val 0.692\n","Epoch 060 | loss 0.1590 | val 0.670\n","Epoch 080 | loss 0.1305 | val 0.673\n","Epoch 100 | loss 0.0800 | val 0.665\n","✅ Final (best-val) | val 0.695 | test 0.688\n","Epoch 000 | loss 0.7216 | val 0.500\n","Epoch 020 | loss 0.3908 | val 0.679\n","Epoch 040 | loss 0.2452 | val 0.682\n","Epoch 060 | loss 0.1512 | val 0.682\n","Epoch 080 | loss 0.1096 | val 0.700\n","Epoch 100 | loss 0.1164 | val 0.692\n","✅ Final (best-val) | val 0.716 | test 0.703\n","Epoch 000 | loss 0.7144 | val 0.500\n","Epoch 020 | loss 0.3853 | val 0.682\n","Epoch 040 | loss 0.2459 | val 0.696\n","Epoch 060 | loss 0.1695 | val 0.693\n","Epoch 080 | loss 0.1236 | val 0.700\n","Epoch 100 | loss 0.1023 | val 0.702\n","✅ Final (best-val) | val 0.713 | test 0.705\n","Epoch 000 | loss 0.7162 | val 0.500\n","Epoch 020 | loss 0.3990 | val 0.701\n","Epoch 040 | loss 0.2544 | val 0.693\n","Epoch 060 | loss 0.1743 | val 0.718\n","Epoch 080 | loss 0.1331 | val 0.702\n","Epoch 100 | loss 0.1130 | val 0.711\n","✅ Final (best-val) | val 0.721 | test 0.687\n","Epoch 000 | loss 0.7288 | val 0.500\n","Epoch 020 | loss 0.4270 | val 0.645\n","Epoch 040 | loss 0.2916 | val 0.652\n","Epoch 060 | loss 0.2044 | val 0.674\n","Epoch 080 | loss 0.1643 | val 0.674\n","Epoch 100 | loss 0.1223 | val 0.695\n","✅ Final (best-val) | val 0.699 | test 0.705\n","Epoch 000 | loss 0.7128 | val 0.500\n","Epoch 020 | loss 0.3689 | val 0.685\n","Epoch 040 | loss 0.2279 | val 0.696\n","Epoch 060 | loss 0.1494 | val 0.708\n","Epoch 080 | loss 0.1078 | val 0.704\n","Epoch 100 | loss 0.0982 | val 0.699\n","✅ Final (best-val) | val 0.715 | test 0.703\n","Epoch 000 | loss 0.6999 | val 0.500\n","Epoch 020 | loss 0.3933 | val 0.718\n","Epoch 040 | loss 0.2551 | val 0.697\n","Epoch 060 | loss 0.1522 | val 0.679\n","Epoch 080 | loss 0.1126 | val 0.676\n","Epoch 100 | loss 0.0987 | val 0.687\n","✅ Final (best-val) | val 0.721 | test 0.699\n","Epoch 000 | loss 0.7144 | val 0.500\n","Epoch 020 | loss 0.3803 | val 0.707\n","Epoch 040 | loss 0.2464 | val 0.695\n","Epoch 060 | loss 0.1711 | val 0.691\n","Epoch 080 | loss 0.1109 | val 0.685\n","Epoch 100 | loss 0.0776 | val 0.671\n","✅ Final (best-val) | val 0.713 | test 0.707\n","Epoch 000 | loss 0.7089 | val 0.500\n","Epoch 020 | loss 0.3720 | val 0.688\n","Epoch 040 | loss 0.2394 | val 0.682\n","Epoch 060 | loss 0.1451 | val 0.670\n","Epoch 080 | loss 0.1033 | val 0.696\n","Epoch 100 | loss 0.0917 | val 0.703\n","✅ Final (best-val) | val 0.713 | test 0.698\n","Epoch 000 | loss 0.7029 | val 0.500\n","Epoch 020 | loss 0.3672 | val 0.685\n","Epoch 040 | loss 0.2187 | val 0.671\n","Epoch 060 | loss 0.1472 | val 0.682\n","Epoch 080 | loss 0.1152 | val 0.679\n","Epoch 100 | loss 0.0960 | val 0.679\n","✅ Final (best-val) | val 0.737 | test 0.714\n","Epoch 000 | loss 0.7149 | val 0.500\n","Epoch 020 | loss 0.4265 | val 0.684\n","Epoch 040 | loss 0.2726 | val 0.691\n","Epoch 060 | loss 0.1622 | val 0.676\n","Epoch 080 | loss 0.1309 | val 0.667\n","Epoch 100 | loss 0.1334 | val 0.673\n","✅ Final (best-val) | val 0.696 | test 0.693\n","Epoch 000 | loss 0.7188 | val 0.500\n","Epoch 020 | loss 0.4264 | val 0.687\n","Epoch 040 | loss 0.2767 | val 0.692\n","Epoch 060 | loss 0.1785 | val 0.699\n","Epoch 080 | loss 0.1216 | val 0.695\n","Epoch 100 | loss 0.1254 | val 0.719\n","✅ Final (best-val) | val 0.721 | test 0.705\n","Epoch 000 | loss 0.7286 | val 0.500\n","Epoch 020 | loss 0.3913 | val 0.667\n","Epoch 040 | loss 0.2404 | val 0.666\n","Epoch 060 | loss 0.1659 | val 0.679\n","Epoch 080 | loss 0.1197 | val 0.708\n","Epoch 100 | loss 0.0979 | val 0.698\n","✅ Final (best-val) | val 0.712 | test 0.681\n","Epoch 000 | loss 0.7178 | val 0.500\n","Epoch 020 | loss 0.3840 | val 0.686\n","Epoch 040 | loss 0.2328 | val 0.670\n","Epoch 060 | loss 0.1647 | val 0.675\n","Epoch 080 | loss 0.1232 | val 0.676\n","Epoch 100 | loss 0.0954 | val 0.666\n","✅ Final (best-val) | val 0.690 | test 0.695\n","Epoch 000 | loss 0.7196 | val 0.500\n","Epoch 020 | loss 0.4184 | val 0.664\n","Epoch 040 | loss 0.2598 | val 0.688\n","Epoch 060 | loss 0.1855 | val 0.680\n","Epoch 080 | loss 0.1348 | val 0.684\n","Epoch 100 | loss 0.1178 | val 0.684\n","✅ Final (best-val) | val 0.702 | test 0.687\n","Epoch 000 | loss 0.7235 | val 0.500\n","Epoch 020 | loss 0.3799 | val 0.677\n","Epoch 040 | loss 0.2535 | val 0.654\n","Epoch 060 | loss 0.1540 | val 0.670\n","Epoch 080 | loss 0.1111 | val 0.663\n","Epoch 100 | loss 0.1020 | val 0.656\n","✅ Final (best-val) | val 0.681 | test 0.678\n","Epoch 000 | loss 0.7019 | val 0.500\n","Epoch 020 | loss 0.3704 | val 0.669\n","Epoch 040 | loss 0.2105 | val 0.676\n","Epoch 060 | loss 0.1366 | val 0.669\n","Epoch 080 | loss 0.1001 | val 0.681\n","Epoch 100 | loss 0.0804 | val 0.679\n","✅ Final (best-val) | val 0.693 | test 0.691\n","Epoch 000 | loss 0.7115 | val 0.500\n","Epoch 020 | loss 0.3803 | val 0.686\n","Epoch 040 | loss 0.2282 | val 0.686\n","Epoch 060 | loss 0.1440 | val 0.707\n","Epoch 080 | loss 0.0867 | val 0.707\n","Epoch 100 | loss 0.0990 | val 0.708\n","✅ Final (best-val) | val 0.721 | test 0.690\n","Epoch 000 | loss 0.7091 | val 0.500\n","Epoch 020 | loss 0.3604 | val 0.696\n","Epoch 040 | loss 0.2346 | val 0.681\n","Epoch 060 | loss 0.1380 | val 0.690\n","Epoch 080 | loss 0.1165 | val 0.696\n","Epoch 100 | loss 0.0868 | val 0.685\n","✅ Final (best-val) | val 0.703 | test 0.692\n","Epoch 000 | loss 0.7241 | val 0.500\n","Epoch 020 | loss 0.4233 | val 0.693\n","Epoch 040 | loss 0.2823 | val 0.690\n","Epoch 060 | loss 0.2080 | val 0.679\n","Epoch 080 | loss 0.1790 | val 0.692\n","Epoch 100 | loss 0.1145 | val 0.701\n","✅ Final (best-val) | val 0.708 | test 0.698\n","Epoch 000 | loss 0.7188 | val 0.500\n","Epoch 020 | loss 0.3960 | val 0.699\n","Epoch 040 | loss 0.2428 | val 0.674\n","Epoch 060 | loss 0.1586 | val 0.698\n","Epoch 080 | loss 0.1226 | val 0.692\n","Epoch 100 | loss 0.0982 | val 0.702\n","✅ Final (best-val) | val 0.721 | test 0.687\n","Epoch 000 | loss 0.7191 | val 0.500\n","Epoch 020 | loss 0.4265 | val 0.691\n","Epoch 040 | loss 0.2757 | val 0.699\n","Epoch 060 | loss 0.2154 | val 0.677\n","Epoch 080 | loss 0.1464 | val 0.674\n","Epoch 100 | loss 0.1133 | val 0.669\n","✅ Final (best-val) | val 0.715 | test 0.690\n","Epoch 000 | loss 0.7040 | val 0.500\n","Epoch 020 | loss 0.3831 | val 0.708\n","Epoch 040 | loss 0.2359 | val 0.695\n","Epoch 060 | loss 0.1639 | val 0.693\n","Epoch 080 | loss 0.1306 | val 0.696\n","Epoch 100 | loss 0.0954 | val 0.692\n","✅ Final (best-val) | val 0.723 | test 0.696\n","Epoch 000 | loss 0.7212 | val 0.500\n","Epoch 020 | loss 0.4388 | val 0.667\n","Epoch 040 | loss 0.2850 | val 0.663\n","Epoch 060 | loss 0.2060 | val 0.662\n","Epoch 080 | loss 0.1406 | val 0.666\n","Epoch 100 | loss 0.1109 | val 0.665\n","✅ Final (best-val) | val 0.689 | test 0.694\n","Epoch 000 | loss 0.7124 | val 0.500\n","Epoch 020 | loss 0.4059 | val 0.692\n","Epoch 040 | loss 0.2475 | val 0.693\n","Epoch 060 | loss 0.1702 | val 0.691\n","Epoch 080 | loss 0.1443 | val 0.700\n","Epoch 100 | loss 0.0941 | val 0.695\n","✅ Final (best-val) | val 0.723 | test 0.685\n","Epoch 000 | loss 0.7150 | val 0.500\n","Epoch 020 | loss 0.4054 | val 0.664\n","Epoch 040 | loss 0.2460 | val 0.686\n","Epoch 060 | loss 0.1689 | val 0.697\n","Epoch 080 | loss 0.1215 | val 0.686\n","Epoch 100 | loss 0.0897 | val 0.682\n","✅ Final (best-val) | val 0.707 | test 0.699\n","Epoch 000 | loss 0.7167 | val 0.500\n","Epoch 020 | loss 0.3832 | val 0.697\n","Epoch 040 | loss 0.2388 | val 0.696\n","Epoch 060 | loss 0.1736 | val 0.693\n","Epoch 080 | loss 0.1417 | val 0.686\n","Epoch 100 | loss 0.0924 | val 0.685\n","✅ Final (best-val) | val 0.708 | test 0.693\n","Epoch 000 | loss 0.7197 | val 0.500\n","Epoch 020 | loss 0.3690 | val 0.690\n","Epoch 040 | loss 0.2243 | val 0.674\n","Epoch 060 | loss 0.1610 | val 0.667\n","Epoch 080 | loss 0.1200 | val 0.668\n","Epoch 100 | loss 0.0881 | val 0.675\n","✅ Final (best-val) | val 0.691 | test 0.688\n","Epoch 000 | loss 0.7123 | val 0.500\n","Epoch 020 | loss 0.3815 | val 0.680\n","Epoch 040 | loss 0.2425 | val 0.681\n","Epoch 060 | loss 0.1583 | val 0.665\n","Epoch 080 | loss 0.1126 | val 0.691\n","Epoch 100 | loss 0.0707 | val 0.691\n","✅ Final (best-val) | val 0.708 | test 0.693\n","Epoch 000 | loss 0.7338 | val 0.500\n","Epoch 020 | loss 0.4062 | val 0.676\n","Epoch 040 | loss 0.2438 | val 0.687\n","Epoch 060 | loss 0.1765 | val 0.677\n","Epoch 080 | loss 0.1400 | val 0.701\n","Epoch 100 | loss 0.1001 | val 0.695\n","✅ Final (best-val) | val 0.707 | test 0.682\n","Epoch 000 | loss 0.7100 | val 0.500\n","Epoch 020 | loss 0.3723 | val 0.691\n","Epoch 040 | loss 0.2447 | val 0.693\n","Epoch 060 | loss 0.1557 | val 0.669\n","Epoch 080 | loss 0.1262 | val 0.658\n","Epoch 100 | loss 0.1082 | val 0.659\n","✅ Final (best-val) | val 0.701 | test 0.694\n","Epoch 000 | loss 0.7137 | val 0.500\n","Epoch 020 | loss 0.3699 | val 0.682\n","Epoch 040 | loss 0.2215 | val 0.688\n","Epoch 060 | loss 0.1470 | val 0.687\n","Epoch 080 | loss 0.1059 | val 0.678\n","Epoch 100 | loss 0.0720 | val 0.689\n","✅ Final (best-val) | val 0.701 | test 0.687\n","Epoch 000 | loss 0.7130 | val 0.500\n","Epoch 020 | loss 0.3437 | val 0.702\n","Epoch 040 | loss 0.2124 | val 0.679\n","Epoch 060 | loss 0.1515 | val 0.687\n","Epoch 080 | loss 0.1100 | val 0.696\n","Epoch 100 | loss 0.1009 | val 0.668\n","✅ Final (best-val) | val 0.721 | test 0.704\n","Epoch 000 | loss 0.7144 | val 0.500\n","Epoch 020 | loss 0.3826 | val 0.696\n","Epoch 040 | loss 0.2579 | val 0.680\n","Epoch 060 | loss 0.1877 | val 0.658\n","Epoch 080 | loss 0.1362 | val 0.673\n","Epoch 100 | loss 0.0882 | val 0.684\n","✅ Final (best-val) | val 0.703 | test 0.697\n","Epoch 000 | loss 0.7516 | val 0.500\n","Epoch 020 | loss 0.4031 | val 0.686\n","Epoch 040 | loss 0.2660 | val 0.675\n","Epoch 060 | loss 0.1802 | val 0.690\n","Epoch 080 | loss 0.1282 | val 0.685\n","Epoch 100 | loss 0.0942 | val 0.681\n","✅ Final (best-val) | val 0.722 | test 0.702\n","Epoch 000 | loss 0.7035 | val 0.500\n","Epoch 020 | loss 0.3686 | val 0.684\n","Epoch 040 | loss 0.2236 | val 0.679\n","Epoch 060 | loss 0.1458 | val 0.697\n","Epoch 080 | loss 0.1012 | val 0.712\n","Epoch 100 | loss 0.1126 | val 0.686\n","✅ Final (best-val) | val 0.719 | test 0.698\n","Epoch 000 | loss 0.7140 | val 0.500\n","Epoch 020 | loss 0.3688 | val 0.695\n","Epoch 040 | loss 0.2383 | val 0.677\n","Epoch 060 | loss 0.1546 | val 0.695\n","Epoch 080 | loss 0.1063 | val 0.692\n","Epoch 100 | loss 0.0812 | val 0.693\n","✅ Final (best-val) | val 0.704 | test 0.707\n","Epoch 000 | loss 0.7211 | val 0.500\n","Epoch 020 | loss 0.4170 | val 0.692\n","Epoch 040 | loss 0.2429 | val 0.697\n","Epoch 060 | loss 0.1551 | val 0.703\n","Epoch 080 | loss 0.1133 | val 0.712\n","Epoch 100 | loss 0.1002 | val 0.721\n","✅ Final (best-val) | val 0.732 | test 0.705\n","Epoch 000 | loss 0.7282 | val 0.500\n","Epoch 020 | loss 0.3910 | val 0.673\n","Epoch 040 | loss 0.2284 | val 0.678\n","Epoch 060 | loss 0.1564 | val 0.695\n","Epoch 080 | loss 0.1162 | val 0.690\n","Epoch 100 | loss 0.1055 | val 0.701\n","✅ Final (best-val) | val 0.704 | test 0.694\n","Epoch 000 | loss 0.7153 | val 0.500\n","Epoch 020 | loss 0.3852 | val 0.656\n","Epoch 040 | loss 0.2487 | val 0.660\n","Epoch 060 | loss 0.1640 | val 0.655\n","Epoch 080 | loss 0.1143 | val 0.669\n","Epoch 100 | loss 0.1167 | val 0.666\n","✅ Final (best-val) | val 0.684 | test 0.690\n","Epoch 000 | loss 0.7278 | val 0.500\n","Epoch 020 | loss 0.3992 | val 0.708\n","Epoch 040 | loss 0.2415 | val 0.679\n","Epoch 060 | loss 0.1698 | val 0.685\n","Epoch 080 | loss 0.1737 | val 0.686\n","Epoch 100 | loss 0.1083 | val 0.693\n","✅ Final (best-val) | val 0.709 | test 0.684\n","Epoch 000 | loss 0.7252 | val 0.500\n","Epoch 020 | loss 0.4003 | val 0.684\n","Epoch 040 | loss 0.2544 | val 0.675\n","Epoch 060 | loss 0.1656 | val 0.674\n","Epoch 080 | loss 0.1367 | val 0.679\n","Epoch 100 | loss 0.1051 | val 0.690\n","✅ Final (best-val) | val 0.700 | test 0.690\n","Epoch 000 | loss 0.7184 | val 0.500\n","Epoch 020 | loss 0.3353 | val 0.692\n","Epoch 040 | loss 0.2044 | val 0.684\n","Epoch 060 | loss 0.1550 | val 0.703\n","Epoch 080 | loss 0.1335 | val 0.712\n","Epoch 100 | loss 0.0908 | val 0.713\n","✅ Final (best-val) | val 0.714 | test 0.715\n","Epoch 000 | loss 0.7057 | val 0.500\n","Epoch 020 | loss 0.3709 | val 0.702\n","Epoch 040 | loss 0.2211 | val 0.714\n","Epoch 060 | loss 0.1421 | val 0.700\n","Epoch 080 | loss 0.1091 | val 0.687\n","Epoch 100 | loss 0.0747 | val 0.680\n","✅ Final (best-val) | val 0.727 | test 0.724\n","Epoch 000 | loss 0.7071 | val 0.500\n","Epoch 020 | loss 0.3648 | val 0.685\n","Epoch 040 | loss 0.2427 | val 0.682\n","Epoch 060 | loss 0.1427 | val 0.682\n","Epoch 080 | loss 0.0971 | val 0.687\n","Epoch 100 | loss 0.0911 | val 0.690\n","✅ Final (best-val) | val 0.707 | test 0.676\n","Epoch 000 | loss 0.7275 | val 0.500\n","Epoch 020 | loss 0.3890 | val 0.690\n","Epoch 040 | loss 0.2415 | val 0.702\n","Epoch 060 | loss 0.1674 | val 0.702\n","Epoch 080 | loss 0.1323 | val 0.701\n","Epoch 100 | loss 0.0950 | val 0.700\n","✅ Final (best-val) | val 0.720 | test 0.710\n","Epoch 000 | loss 0.7083 | val 0.500\n","Epoch 020 | loss 0.3493 | val 0.681\n","Epoch 040 | loss 0.2180 | val 0.691\n","Epoch 060 | loss 0.1530 | val 0.704\n","Epoch 080 | loss 0.1039 | val 0.704\n","Epoch 100 | loss 0.1028 | val 0.697\n","✅ Final (best-val) | val 0.716 | test 0.702\n","F+ train/test: 50/50 | F- train/test: 50/50 (totals match budgets)\n"]}]},{"cell_type":"code","source":["#=======================================================\n","# Fingerprint set for edge-level (P fingerprints, sample m pairs per graph)\n","#=======================================================\n","class FingerprintGraph(nn.Module):\n","    def __init__(self, n_nodes, feat_dim, sample_m, edge_init_p=0.05):\n","        super().__init__()\n","        self.n = n_nodes\n","        self.d = feat_dim\n","        self.m = min(sample_m, n_nodes*(n_nodes-1)//2)  # Max possible pairs\n","        X = torch.empty(self.n, self.d).uniform_(-0.5, 0.5)\n","        self.X = nn.Parameter(X.to(device))\n","        A0 = (torch.rand(self.n, self.n, device=device) < edge_init_p).float()\n","        A0.fill_diagonal_(0.0)\n","        A0 = torch.maximum(A0, A0.T)\n","        self.A_logits = nn.Parameter(torch.logit(torch.clamp(A0, 1e-4, 1-1e-4)))\n","        #Sample m unique node pairs (for potential edges)\n","        all_pairs = torch.combinations(torch.arange(self.n, device=device), r=2)\n","        perm = torch.randperm(len(all_pairs), device=device)[:self.m]\n","        self.sample_pairs = all_pairs[perm].t()  # 2 x m\n","\n","    @torch.no_grad()\n","    def edge_index(self):\n","        A_prob = torch.sigmoid(self.A_logits)\n","        A_bin = (A_prob > 0.5).float()\n","        A_bin.fill_diagonal_(0.0)\n","        A_bin = torch.maximum(A_bin, A_bin.T)\n","        idx = A_bin.nonzero(as_tuple=False)\n","        if idx.numel() == 0:\n","            return torch.empty(2, 0, dtype=torch.long, device=device)\n","        return idx.t().contiguous()\n","\n","    @torch.no_grad()\n","    def flip_topk_by_grad(self, gradA, topk=64, step=2.5):\n","        g = gradA.abs()\n","        triu = torch.triu(torch.ones_like(g), diagonal=1)\n","        scores = (g * triu).flatten()\n","        k = min(topk, scores.numel())\n","        if k == 0: return\n","        _, idxs = torch.topk(scores, k=k)\n","        r = self.n\n","        pairs = torch.stack((idxs // r, idxs % r), dim=1)\n","        A_prob = torch.sigmoid(self.A_logits).detach()\n","        for (u, v) in pairs.tolist():\n","            guv = gradA[u, v].item()\n","            exist = A_prob[u, v] > 0.5\n","            if exist and guv <= 0:\n","                self.A_logits.data[u, v] -= step\n","                self.A_logits.data[v, u] -= step\n","            elif (not exist) and guv >= 0:\n","                self.A_logits.data[u, v] += step\n","                self.A_logits.data[v, u] += step\n","        self.A_logits.data.fill_diagonal_(-10.0)\n","\n","class FingerprintSet(nn.Module):\n","    def __init__(self, P, n_nodes, feat_dim, sample_m, edge_init_p=0.05, topk_edges=64, edge_step=2.5):\n","        super().__init__()\n","        self.P = P\n","        self.fps = nn.ModuleList([FingerprintGraph(n_nodes, feat_dim, sample_m, edge_init_p) for _ in range(P)]).to(device)\n","        self.topk_edges = topk_edges\n","        self.edge_step = edge_step\n","\n","    def concat_outputs(self, model, *, require_grad: bool = False):\n","        outs = []\n","        model.eval()\n","        ctx = torch.enable_grad() if require_grad else torch.no_grad()\n","        with ctx:\n","            for fp in self.fps:\n","                ei = fp.edge_index()\n","                h  = model(fp.X, ei)\n","                h_u = h[fp.sample_pairs[0]]\n","                h_v = h[fp.sample_pairs[1]]\n","                probs = torch.sigmoid((h_u * h_v).sum(dim=-1))  # m probabilities\n","                outs.append(probs)\n","        return torch.cat(outs, dim=0)\n","\n","    def flip_adj_by_grad(self, surrogate_grad_list):\n","        for fp, g in zip(self.fps, surrogate_grad_list):\n","            fp.flip_topk_by_grad(g, topk=self.topk_edges, step=self.edge_step)\n","\n","fp_set = FingerprintSet(\n","    P=CFG[\"FP_P\"],\n","    n_nodes=CFG[\"FP_NODES\"],\n","    feat_dim=num_feats,\n","    sample_m=CFG[\"FP_SAMPLE_M\"],\n","    edge_init_p=CFG[\"FP_EDGE_INIT_P\"],\n","    topk_edges=CFG[\"FP_EDGE_TOPK\"],\n","    edge_step=CFG[\"EDGE_LOGIT_STEP\"],\n",")\n","INPUT_DIM = CFG[\"FP_P\"] * CFG[\"FP_SAMPLE_M\"]  # P graphs * m pairs * 1 (prob)\n","print(\"Univerifier input dim =\", INPUT_DIM)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vs33uzf9qE9P","executionInfo":{"status":"ok","timestamp":1756065539752,"user_tz":-360,"elapsed":104,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"b5981e55-6dd8-4f72-9dbe-c1cd111ade45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Univerifier input dim = 2048\n"]}]},{"cell_type":"code","source":["#========================================\n","# Univerifier (binary classifier)\n","#========================================\n","class Univerifier(nn.Module):\n","    def __init__(self, input_dim: int):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 128), nn.LeakyReLU(0.01),\n","            nn.Linear(128, 64),       nn.LeakyReLU(0.01),\n","            nn.Linear(64, 32),        nn.LeakyReLU(0.01),\n","            nn.Linear(32, 2),\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","V = Univerifier(INPUT_DIM).to(device)\n","opt_V = Adam(V.parameters(), lr=CFG[\"LR_V\"])"],"metadata":{"id":"yCKCUkgkqcAU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#=====================================================\n","# Joint learning (Algorithm-1 for edge-level)\n","#=====================================================\n","models_pos_tr = [model_f.to(device)] + [m.to(device) for m in F_pos_tr]\n","models_neg_tr = [m.to(device) for m in F_neg_tr]\n","print(f\"Train pools -> Pos: {len(models_pos_tr)} | Neg: {len(models_neg_tr)}\")\n","def batch_from_pool(fp_set, pos_models, neg_models, *, require_grad: bool):\n","    X = []; y = []\n","    for m in pos_models:\n","        X.append(fp_set.concat_outputs(m, require_grad=require_grad)); y.append(1)\n","    for m in neg_models:\n","        X.append(fp_set.concat_outputs(m, require_grad=require_grad)); y.append(0)\n","    return torch.stack(X, dim=0), torch.tensor(y, device=device)\n","def surrogate_grad_A_for_fp(fp, model):\n","    #cosine-sim surrogate gradient from target model's first layer\n","    with torch.no_grad():\n","        ei = fp.edge_index()\n","        h  = model.conv1(fp.X, ei)\n","        h  = F.relu(h)\n","        hn = F.normalize(h, dim=-1)\n","        sim = hn @ hn.t()\n","        gradA = sim - 0.5\n","    return gradA.detach().cpu()\n","def update_features(fp_set, V, pos_models, neg_models, steps, lr_x):\n","    #freeze model params\n","    for m in pos_models + neg_models:\n","        for p in m.parameters(): p.requires_grad_(False)\n","    #turn on grad for X before building batch\n","    for fp in fp_set.fps:\n","        fp.X.requires_grad_(True)\n","    for _ in range(steps):\n","        Xb, yb = batch_from_pool(fp_set, pos_models, neg_models, require_grad=True)\n","        V.eval()\n","        for p in V.parameters(): p.requires_grad_(False)\n","        logits = V(Xb.to(device))\n","        loss = F.cross_entropy(logits, yb)\n","        for fp in fp_set.fps:\n","            if fp.X.grad is not None: fp.X.grad.zero_()\n","        loss.backward()\n","        with torch.no_grad():\n","            for fp in fp_set.fps:\n","                fp.X.add_(lr_x * fp.X.grad)\n","                fp.X.grad.zero_()\n","        for p in V.parameters(): p.requires_grad_(True)\n","    grads = [surrogate_grad_A_for_fp(fp, pos_models[0]) for fp in fp_set.fps]\n","    fp_set.flip_adj_by_grad(grads)\n","def update_verifier(fp_set, V, pos_models, neg_models, steps):\n","    for _ in range(steps):\n","        V.train()\n","        Xb, yb = batch_from_pool(fp_set, pos_models, neg_models, require_grad=False)\n","        logits = V(Xb.to(device))\n","        loss = F.cross_entropy(logits, yb)\n","        opt_V.zero_grad(); loss.backward(); opt_V.step()\n","for it in range(1, CFG[\"OUTER_ITERS\"] + 1):\n","    update_features(fp_set, V, models_pos_tr, models_neg_tr, steps=CFG[\"FP_STEPS\"], lr_x=CFG[\"LR_X\"])\n","    update_verifier(fp_set, V, models_pos_tr, models_neg_tr, steps=CFG[\"V_STEPS\"])\n","    V.eval()\n","    Xb, yb = batch_from_pool(fp_set, models_pos_tr, models_neg_tr, require_grad=False)\n","    with torch.no_grad():\n","        pred = V(Xb).argmax(dim=1)\n","        acc = (pred.cpu() == yb.cpu()).float().mean().item()\n","        pos_acc = (pred[:len(models_pos_tr)].cpu() == 1).float().mean().item()\n","        neg_acc = (pred[len(models_pos_tr):].cpu() == 0).float().mean().item()\n","    print(f\"Iter {it:02d}/{CFG['OUTER_ITERS']} | train all {acc:.3f} | pos {pos_acc:.3f} | neg {neg_acc:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IFNSdJA7qpGH","executionInfo":{"status":"ok","timestamp":1756070366450,"user_tz":-360,"elapsed":4673235,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"13764af1-1688-4f1b-a1da-8d13bb6c06cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train pools -> Pos: 51 | Neg: 50\n","Iter 01/20 | train all 0.495 | pos 0.000 | neg 1.000\n","Iter 02/20 | train all 0.782 | pos 1.000 | neg 0.560\n","Iter 03/20 | train all 0.822 | pos 1.000 | neg 0.640\n","Iter 04/20 | train all 0.960 | pos 0.941 | neg 0.980\n","Iter 05/20 | train all 0.891 | pos 1.000 | neg 0.780\n","Iter 06/20 | train all 0.921 | pos 1.000 | neg 0.840\n","Iter 07/20 | train all 0.970 | pos 0.961 | neg 0.980\n","Iter 08/20 | train all 0.980 | pos 1.000 | neg 0.960\n","Iter 09/20 | train all 0.980 | pos 1.000 | neg 0.960\n","Iter 10/20 | train all 0.941 | pos 0.882 | neg 1.000\n","Iter 11/20 | train all 0.941 | pos 1.000 | neg 0.880\n","Iter 12/20 | train all 0.990 | pos 1.000 | neg 0.980\n","Iter 13/20 | train all 1.000 | pos 1.000 | neg 1.000\n","Iter 14/20 | train all 1.000 | pos 1.000 | neg 1.000\n","Iter 15/20 | train all 1.000 | pos 1.000 | neg 1.000\n","Iter 16/20 | train all 1.000 | pos 1.000 | neg 1.000\n","Iter 17/20 | train all 1.000 | pos 1.000 | neg 1.000\n","Iter 18/20 | train all 0.812 | pos 1.000 | neg 0.620\n","Iter 19/20 | train all 0.921 | pos 0.843 | neg 1.000\n","Iter 20/20 | train all 0.990 | pos 0.980 | neg 1.000\n"]}]},{"cell_type":"code","source":["# Held-out verification (Robustness/Uniqueness/ARUC)\n","#==========================================================\n","models_pos_te = [model_f.to(device)] + [m.to(device) for m in F_pos_te]\n","models_neg_te = [m.to(device) for m in F_neg_te]\n","@torch.no_grad()\n","def verify_scores(V, fp_set, models):\n","    Xs = [fp_set.concat_outputs(m, require_grad=False) for m in models]\n","    logits = V(torch.stack(Xs, dim=0).to(device))\n","    probs = F.softmax(logits, dim=-1)[:, 1]  # p(positive)\n","    return probs.detach().cpu().numpy()\n","def sweep_threshold(p_pos, p_neg, num=301):\n","    ths = np.linspace(0.0, 1.0, num=num)\n","    R = []; U = []; A = []\n","    for t in ths:\n","        tp = (p_pos >= t).mean()    # robustness\n","        tn = (p_neg <  t).mean()    # uniqueness\n","        R.append(tp); U.append(tn)\n","        A.append((tp + tn) / 2.0)   # balanced acc (pos/neg equal)\n","    return ths, np.array(R), np.array(U), np.array(A)\n","p_pos = verify_scores(V, fp_set, models_pos_te)\n","p_neg = verify_scores(V, fp_set, models_neg_te)\n","ths, R, U, A = sweep_threshold(p_pos, p_neg, num=301)\n","best_idx = A.argmax()\n","mean_acc = A.mean()\n","#numpy>=2.0 has trapezoid; fallback to trapz if unavailable\n","ARUC = np.trapezoid(np.minimum(R, U), ths) if hasattr(np, \"trapezoid\") else np.trapz(np.minimum(R, U), ths)\n","print(f\"Best @ λ={ths[best_idx]:.3f} | Robustness={R[best_idx]:.3f} | Uniqueness={U[best_idx]:.3f} | MeanAcc*={A[best_idx]:.3f}\")\n","print(f\"Mean Test Accuracy (avg over λ): {mean_acc:.3f}\")\n","print(f\"ARUC (approx): {ARUC:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fubfI2F53cZW","executionInfo":{"status":"ok","timestamp":1756070401509,"user_tz":-360,"elapsed":12707,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"cc6be137-c392-44c8-e66d-cc4b83e47eec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best @ λ=0.387 | Robustness=0.922 | Uniqueness=1.000 | MeanAcc*=0.961\n","Mean Test Accuracy (avg over λ): 0.912\n","ARUC (approx): 0.835\n"]}]}]}