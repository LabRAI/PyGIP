{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03nySo9dNEQU","executionInfo":{"status":"ok","timestamp":1756143678163,"user_tz":-360,"elapsed":17355,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"413539f2-2ab0-4751-e010-d1dd405cb544"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ PyTorch & PyG install attempted.\n"]}],"source":["# ================================\n","#  Environment setup (Colab)\n","# ================================\n","!pip -q install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n","!pip -q install torch-geometric torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n","print(\"✅ PyTorch & PyG install attempted.\")"]},{"cell_type":"code","source":["# ================================\n","#  Imports & Utilities\n","# ================================\n","import os, random, copy\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from torch_geometric.datasets import TUDataset\n","from torch_geometric.nn import GCNConv, SAGEConv, global_mean_pool, global_add_pool\n","from torch_geometric.loader import DataLoader\n","from torch_geometric.utils import to_dense_batch\n","from sklearn.metrics import mean_squared_error\n","\n","# Device & seed\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Device:\", device)\n","\n","def set_seed(seed: int = 42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P4p8RABYNgHf","executionInfo":{"status":"ok","timestamp":1756143678226,"user_tz":-360,"elapsed":42,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"fc97f617-2b55-4a77-9c20-d7454d9b7b7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cpu\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Load AIDS dataset for graph matching\n","# ================================\n","# Load AIDS dataset\n","dataset = TUDataset(root='/content/data/AIDS', name='AIDS')\n","print(f\"Dataset: {dataset}\")\n","print(f\"Number of graphs: {len(dataset)}\")\n","print(f\"Number of features: {dataset.num_features}\")\n","print(f\"Number of classes: {dataset.num_classes}\")\n","\n","# Create graph pairs for matching task\n","def create_graph_pairs(dataset, num_pairs=1000, seed=42):\n","    \"\"\"Create pairs of graphs with similarity scores for matching task\"\"\"\n","    set_seed(seed)\n","    pairs = []\n","    similarities = []\n","\n","    for _ in range(num_pairs):\n","        # Randomly select two graphs\n","        idx1, idx2 = torch.randint(0, len(dataset), (2,)).tolist()\n","        graph1 = dataset[idx1]\n","        graph2 = dataset[idx2]\n","\n","        # Simple similarity based on class labels and graph size\n","        if graph1.y.item() == graph2.y.item():\n","            # Same class: higher similarity\n","            base_sim = 0.7 + 0.3 * torch.rand(1).item()\n","        else:\n","            # Different class: lower similarity\n","            base_sim = 0.1 + 0.4 * torch.rand(1).item()\n","\n","        # Adjust based on size difference\n","        size_diff = abs(graph1.num_nodes - graph2.num_nodes) / max(graph1.num_nodes, graph2.num_nodes)\n","        adjusted_sim = base_sim * (1 - 0.3 * size_diff)\n","\n","        pairs.append((graph1, graph2))\n","        similarities.append(adjusted_sim)\n","\n","    return pairs, similarities\n","\n","# Create training, validation, and test pairs\n","train_pairs, train_sims = create_graph_pairs(dataset, num_pairs=800, seed=1)\n","val_pairs, val_sims = create_graph_pairs(dataset, num_pairs=100, seed=2)\n","test_pairs, test_sims = create_graph_pairs(dataset, num_pairs=100, seed=3)\n","\n","num_feats = dataset.num_features\n","print(f\"Features: {num_feats}\")\n","print(f\"Train/Val/Test pairs: {len(train_pairs)}/{len(val_pairs)}/{len(test_pairs)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4P1ZyLJHP28o","executionInfo":{"status":"ok","timestamp":1756143679272,"user_tz":-360,"elapsed":1043,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"5228f698-8497-4b50-c48f-fde87575b912"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset: AIDS(2000)\n","Number of graphs: 2000\n","Number of features: 38\n","Number of classes: 2\n","Features: 38\n","Train/Val/Test pairs: 800/100/100\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Config for AIDS graph matching\n","# ================================\n","CFG = dict(\n","    POS_TRAIN=15, POS_TEST=15,    # Reduced from 30\n","    NEG_TRAIN=15, NEG_TEST=15,    # Reduced from 30\n","    USE_FT_LAST=True,\n","    USE_FT_ALL=False,\n","    USE_PR_LAST=False,\n","    USE_PR_ALL=False,\n","    USE_DISTILL=False,            # Disable first\n","    DISTILL_STEPS=50,\n","    FP_P=32,                      # Reduced from 64\n","    FP_NODES=16,\n","    FP_EDGE_INIT_P=0.1,\n","    FP_EDGE_TOPK=24,\n","    EDGE_LOGIT_STEP=2.0,\n","    OUTER_ITERS=15,               # Slightly increased\n","    FP_STEPS=2,\n","    V_STEPS=3,\n","    LR_TARGET=0.005,\n","    WD_TARGET=5e-4,\n","    LR_V=1e-3,\n","    LR_X=1e-3,\n","    SEED=1,\n",")\n","print(CFG)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82ZYFINlO0aA","executionInfo":{"status":"ok","timestamp":1756161941950,"user_tz":-360,"elapsed":38,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"317b2ab9-ebf1-420e-fce3-978c0d498b85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'POS_TRAIN': 15, 'POS_TEST': 15, 'NEG_TRAIN': 15, 'NEG_TEST': 15, 'USE_FT_LAST': True, 'USE_FT_ALL': False, 'USE_PR_LAST': False, 'USE_PR_ALL': False, 'USE_DISTILL': False, 'DISTILL_STEPS': 50, 'FP_P': 32, 'FP_NODES': 16, 'FP_EDGE_INIT_P': 0.1, 'FP_EDGE_TOPK': 24, 'EDGE_LOGIT_STEP': 2.0, 'OUTER_ITERS': 15, 'FP_STEPS': 2, 'V_STEPS': 3, 'LR_TARGET': 0.005, 'WD_TARGET': 0.0005, 'LR_V': 0.001, 'LR_X': 0.001, 'SEED': 1}\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Define GNN models for graph matching\n","# ================================\n","class GCN(nn.Module):\n","    def __init__(self, in_channels, hidden, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = GCNConv(in_channels, hidden)\n","        self.conv2 = GCNConv(hidden, hidden)\n","        self.conv3 = GCNConv(hidden, hidden)\n","        self.dropout = dropout\n","\n","        # Graph matching specific layers\n","        self.matching_head = nn.Sequential(\n","            nn.Linear(hidden * 2, hidden),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x1, edge_index1, batch1, x2, edge_index2, batch2):\n","        # Process first graph\n","        h1 = self.conv1(x1, edge_index1)\n","        h1 = F.relu(h1)\n","        h1 = F.dropout(h1, p=self.dropout, training=self.training)\n","        h1 = self.conv2(h1, edge_index1)\n","        h1 = F.relu(h1)\n","        h1 = F.dropout(h1, p=self.dropout, training=self.training)\n","        h1 = self.conv3(h1, edge_index1)\n","\n","        # Process second graph\n","        h2 = self.conv1(x2, edge_index2)\n","        h2 = F.relu(h2)\n","        h2 = F.dropout(h2, p=self.dropout, training=self.training)\n","        h2 = self.conv2(h2, edge_index2)\n","        h2 = F.relu(h2)\n","        h2 = F.dropout(h2, p=self.dropout, training=self.training)\n","        h2 = self.conv3(h2, edge_index2)\n","\n","        # Global pooling\n","        g1 = global_mean_pool(h1, batch1)\n","        g2 = global_mean_pool(h2, batch2)\n","\n","        # Concatenate and predict similarity\n","        combined = torch.cat([g1, g2], dim=1)\n","        similarity = self.matching_head(combined)\n","\n","        return similarity.squeeze()\n","\n","class GraphSAGE(nn.Module):\n","    def __init__(self, in_channels, hidden, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden)\n","        self.conv2 = SAGEConv(hidden, hidden)\n","        self.conv3 = SAGEConv(hidden, hidden)\n","        self.dropout = dropout\n","\n","        # Graph matching specific layers\n","        self.matching_head = nn.Sequential(\n","            nn.Linear(hidden * 2, hidden),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x1, edge_index1, batch1, x2, edge_index2, batch2):\n","        # Process first graph\n","        h1 = self.conv1(x1, edge_index1)\n","        h1 = F.relu(h1)\n","        h1 = F.dropout(h1, p=self.dropout, training=self.training)\n","        h1 = self.conv2(h1, edge_index1)\n","        h1 = F.relu(h1)\n","        h1 = F.dropout(h1, p=self.dropout, training=self.training)\n","        h1 = self.conv3(h1, edge_index1)\n","\n","        # Process second graph\n","        h2 = self.conv1(x2, edge_index2)\n","        h2 = F.relu(h2)\n","        h2 = F.dropout(h2, p=self.dropout, training=self.training)\n","        h2 = self.conv2(h2, edge_index2)\n","        h2 = F.relu(h2)\n","        h2 = F.dropout(h2, p=self.dropout, training=self.training)\n","        h2 = self.conv3(h2, edge_index2)\n","\n","        # Global pooling\n","        g1 = global_mean_pool(h1, batch1)\n","        g2 = global_mean_pool(h2, batch2)\n","\n","        # Concatenate and predict similarity\n","        combined = torch.cat([g1, g2], dim=1)\n","        similarity = self.matching_head(combined)\n","\n","        return similarity.squeeze()\n","\n"],"metadata":{"id":"qeufkKJQP6kP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================\n","#  Training helpers for graph matching\n","# ================================\n","def create_batch_from_pairs(pairs, similarities, batch_size=32):\n","    \"\"\"Create batches from graph pairs\"\"\"\n","    batches = []\n","    for i in range(0, len(pairs), batch_size):\n","        batch_pairs = pairs[i:i+batch_size]\n","        batch_sims = similarities[i:i+batch_size]\n","        batches.append((batch_pairs, batch_sims))\n","    return batches\n","\n","@torch.no_grad()\n","def evaluate_matching(model, pairs, similarities):\n","    model.eval()\n","    all_preds = []\n","    all_targets = []\n","\n","    for pair, sim in zip(pairs, similarities):\n","        g1, g2 = pair\n","        g1, g2 = g1.to(device), g2.to(device)\n","\n","        # Create batch indices (single pair)\n","        batch1 = torch.zeros(g1.num_nodes, dtype=torch.long, device=device)\n","        batch2 = torch.zeros(g2.num_nodes, dtype=torch.long, device=device)\n","\n","        pred = model(g1.x, g1.edge_index, batch1, g2.x, g2.edge_index, batch2)\n","        all_preds.append(pred.item())\n","        all_targets.append(sim)\n","\n","    mse = mean_squared_error(all_targets, all_preds)\n","    return 1.0 / (1.0 + mse)  # Convert MSE to accuracy-like score\n","\n","def train_graph_matching(model, train_pairs, train_sims, val_pairs, val_sims, epochs=200, lr=0.001, wd=5e-4):\n","    model = model.to(device)\n","    opt = Adam(model.parameters(), lr=lr, weight_decay=wd)\n","    best = {'val': 0.0, 'state': None}\n","\n","    for ep in range(epochs):\n","        model.train()\n","        total_loss = 0\n","\n","        # Process in mini-batches\n","        batches = create_batch_from_pairs(train_pairs, train_sims, batch_size=16)\n","\n","        for batch_pairs, batch_sims in batches:\n","            opt.zero_grad()\n","            batch_loss = 0\n","\n","            for pair, sim in zip(batch_pairs, batch_sims):\n","                g1, g2 = pair\n","                g1, g2 = g1.to(device), g2.to(device)\n","\n","                # Create batch indices (single pair)\n","                batch1 = torch.zeros(g1.num_nodes, dtype=torch.long, device=device)\n","                batch2 = torch.zeros(g2.num_nodes, dtype=torch.long, device=device)\n","\n","                pred = model(g1.x, g1.edge_index, batch1, g2.x, g2.edge_index, batch2)\n","                target = torch.tensor(sim, device=device, dtype=torch.float)\n","                loss = F.mse_loss(pred, target)\n","                batch_loss += loss\n","\n","            batch_loss /= len(batch_pairs)\n","            batch_loss.backward()\n","            opt.step()\n","            total_loss += batch_loss.item()\n","\n","        # Validation\n","        va = evaluate_matching(model, val_pairs, val_sims)\n","        if va > best['val']:\n","            best['val'] = va\n","            best['state'] = copy.deepcopy(model.state_dict())\n","\n","        if ep % 40 == 0:\n","            print(f\"Epoch {ep:03d} | loss {total_loss:.4f} | val {va:.3f}\")\n","\n","    if best['state'] is not None:\n","        model.load_state_dict(best['state'])\n","\n","    te = evaluate_matching(model, test_pairs, test_sims)\n","    print(f\"✅ Final (best-val) | val {best['val']:.3f} | test {te:.3f}\")\n","    return model\n"],"metadata":{"id":"7Mn_1MejQDeb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================\n","#  Train target model f (GCN for graph matching)\n","# ================================\n","set_seed(CFG[\"SEED\"])\n","model_f = GCN(num_feats, hidden=16, dropout=0.5) #32\n","model_f = train_graph_matching(model_f, train_pairs, train_sims, val_pairs, val_sims,\n","                              epochs=60, lr=CFG[\"LR_TARGET\"], wd=CFG[\"WD_TARGET\"])   #200\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CGFg_msuQHe6","executionInfo":{"status":"ok","timestamp":1756162231012,"user_tz":-360,"elapsed":274123,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"8ca7a63f-607b-4641-91c1-8e100af539b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 000 | loss 3.9272 | val 0.929\n","Epoch 040 | loss 3.7616 | val 0.928\n","✅ Final (best-val) | val 0.929 | test 0.925\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Build suspect models (F+ and F−) for graph matching\n","# ================================\n","@torch.no_grad()\n","def reset_module(m):\n","    for layer in m.modules():\n","        if hasattr(layer, 'reset_parameters'):\n","            layer.reset_parameters()\n","\n","def ft_model(base_model, train_pairs, train_sims, last_only=True, epochs=10, lr=0.001, seed=123):\n","    set_seed(seed)\n","    m = copy.deepcopy(base_model).to(device)\n","\n","    # Freeze/unfreeze parameters\n","    for p in m.parameters():\n","        p.requires_grad_(not last_only)\n","    for p in m.conv3.parameters():\n","        p.requires_grad_(True)  # last conv layer\n","    for p in m.matching_head.parameters():\n","        p.requires_grad_(True)  # matching head\n","\n","    opt = Adam(filter(lambda p: p.requires_grad, m.parameters()), lr=lr)\n","\n","    for _ in range(epochs):\n","        m.train()\n","        for pair, sim in zip(train_pairs[:20], train_sims[:20]):  # Subset for speed\n","            g1, g2 = pair\n","            g1, g2 = g1.to(device), g2.to(device)\n","\n","            batch1 = torch.zeros(g1.num_nodes, dtype=torch.long, device=device)\n","            batch2 = torch.zeros(g2.num_nodes, dtype=torch.long, device=device)\n","\n","            opt.zero_grad()\n","            pred = m(g1.x, g1.edge_index, batch1, g2.x, g2.edge_index, batch2)\n","            target = torch.tensor(sim, device=device, dtype=torch.float)\n","            loss = F.mse_loss(pred, target)\n","            loss.backward()\n","            opt.step()\n","\n","    return m.eval()\n","\n","def pr_model(base_model, train_pairs, train_sims, last_only=True, epochs=10, lr=0.001, seed=456):\n","    set_seed(seed)\n","    m = copy.deepcopy(base_model).to(device)\n","\n","    if last_only:\n","        reset_module(m.conv3)\n","        reset_module(m.matching_head)\n","    else:\n","        reset_module(m)\n","\n","    opt = Adam(m.parameters(), lr=lr)\n","\n","    for _ in range(epochs):\n","        m.train()\n","        for pair, sim in zip(train_pairs[:20], train_sims[:20]):  # Subset for speed\n","            g1, g2 = pair\n","            g1, g2 = g1.to(device), g2.to(device)\n","\n","            batch1 = torch.zeros(g1.num_nodes, dtype=torch.long, device=device)\n","            batch2 = torch.zeros(g2.num_nodes, dtype=torch.long, device=device)\n","\n","            opt.zero_grad()\n","            pred = m(g1.x, g1.edge_index, batch1, g2.x, g2.edge_index, batch2)\n","            target = torch.tensor(sim, device=device, dtype=torch.float)\n","            loss = F.mse_loss(pred, target)\n","            loss.backward()\n","            opt.step()\n","\n","    return m.eval()\n","\n","def make_student(arch='GCN', hidden=16):\n","    return (GCN(num_feats, hidden, dropout=0.5).to(device)\n","            if arch=='GCN' else\n","            GraphSAGE(num_feats, hidden, dropout=0.5).to(device))\n","\n","def distill_from_teacher(teacher, train_pairs, arch='GCN', steps=250, lr=0.01, seed=777):\n","    set_seed(seed)\n","    student = make_student(arch, hidden=16)\n","    opt = Adam(student.parameters(), lr=lr)\n","    mse = nn.MSELoss()\n","\n","    for t in range(steps):\n","        idx = torch.randint(0, len(train_pairs), (1,)).item()\n","        pair = train_pairs[idx]\n","        g1, g2 = pair\n","        g1, g2 = g1.to(device), g2.to(device)\n","\n","        batch1 = torch.zeros(g1.num_nodes, dtype=torch.long, device=device)\n","        batch2 = torch.zeros(g2.num_nodes, dtype=torch.long, device=device)\n","\n","        with torch.no_grad():\n","            teacher_out = teacher(g1.x, g1.edge_index, batch1, g2.x, g2.edge_index, batch2)\n","\n","        student.train()\n","        opt.zero_grad()\n","        student_out = student(g1.x, g1.edge_index, batch1, g2.x, g2.edge_index, batch2)\n","        loss = mse(student_out, teacher_out)\n","        loss.backward()\n","        opt.step()\n","\n","    return student.eval()\n","\n","def _distribute_budget(total, keys):\n","    if not keys: return {}\n","    base = total // len(keys)\n","    rem = total - base * len(keys)\n","    out = {k: base for k in keys}\n","    for k in keys[:rem]:\n","        out[k] += 1\n","    return out\n","\n","# ===== Generate Positives (F+) =====\n","F_pos_all = []\n","pos_total = CFG[\"POS_TRAIN\"] + CFG[\"POS_TEST\"]\n","pos_keys = []\n","if CFG[\"USE_FT_LAST\"]: pos_keys.append(\"FT_LAST\")\n","if CFG[\"USE_FT_ALL\"]:  pos_keys.append(\"FT_ALL\")\n","if CFG[\"USE_PR_LAST\"]: pos_keys.append(\"PR_LAST\")\n","if CFG[\"USE_PR_ALL\"]:  pos_keys.append(\"PR_ALL\")\n","if CFG[\"USE_DISTILL\"]: pos_keys.append(\"DISTILL\")\n","\n","pos_budget = _distribute_budget(pos_total, pos_keys)\n","seed_base = 10\n","\n","for key in pos_keys:\n","    cnt = pos_budget[key]\n","    if key == \"FT_LAST\":\n","        for s in range(seed_base, seed_base+cnt):\n","            F_pos_all.append(ft_model(model_f, train_pairs, train_sims, last_only=True, epochs=10, seed=s))\n","        seed_base += cnt\n","    elif key == \"FT_ALL\":\n","        for s in range(seed_base, seed_base+cnt):\n","            F_pos_all.append(ft_model(model_f, train_pairs, train_sims, last_only=False, epochs=10, seed=s))\n","        seed_base += cnt\n","    elif key == \"PR_LAST\":\n","        for s in range(seed_base, seed_base+cnt):\n","            F_pos_all.append(pr_model(model_f, train_pairs, train_sims, last_only=True, epochs=10, seed=s))\n","        seed_base += cnt\n","    elif key == \"PR_ALL\":\n","        for s in range(seed_base, seed_base+cnt):\n","            F_pos_all.append(pr_model(model_f, train_pairs, train_sims, last_only=False, epochs=10, seed=s))\n","        seed_base += cnt\n","    elif key == \"DISTILL\":\n","        arches = (['GCN'] * (cnt//2) + ['SAGE'] * (cnt - cnt//2))\n","        for i, arch in enumerate(arches, 400):\n","            F_pos_all.append(distill_from_teacher(model_f, train_pairs, arch=arch,\n","                                                steps=CFG[\"DISTILL_STEPS\"], seed=1000+i))\n","\n","print(f\"Generated {len(F_pos_all)} positive models\")\n","\n","# ===== Generate Negatives (F−) =====\n","F_neg_all = []\n","neg_total = CFG[\"NEG_TRAIN\"] + CFG[\"NEG_TEST\"]\n","\n","# 🔥🔥🔥 ULTIMATE FIX: Make negatives predict OPPOSITE behavior\n","def create_opposite_similarities(pairs, original_sims, noise_level=0.3):\n","    opposite_sims = []\n","    for sim in original_sims:\n","        # Flip the similarity: 0.8 becomes 0.2, 0.3 becomes 0.7, etc.\n","        opposite_sim = 1.0 - sim\n","        # Add some noise to make it natural\n","        noisy_sim = opposite_sim + (torch.randn(1).item() * noise_level)\n","        # Clip to valid range [0, 1]\n","        noisy_sim = max(0.1, min(0.9, noisy_sim))\n","        opposite_sims.append(noisy_sim)\n","    return opposite_sims\n","\n","# Create OPPOSITE similarities for negative models\n","opposite_train_sims = create_opposite_similarities(train_pairs, train_sims, noise_level=0.2)\n","opposite_val_sims = create_opposite_similarities(val_pairs, val_sims, noise_level=0.2)\n","\n","for s in range(500, 500 + neg_total):\n","    set_seed(s)\n","    m = GCN(num_feats, 32, dropout=0.5)\n","    # 🔥🔥🔥 Train negatives to predict OPPOSITE similarities\n","    m = train_graph_matching(m, train_pairs, opposite_train_sims, val_pairs, opposite_val_sims,\n","                           epochs=50, lr=0.001, wd=5e-4)\n","    F_neg_all.append(m.eval())\n","\n","print(f\"Generated {len(F_neg_all)} negative models\")\n","\n","# ===== Train/Test split =====\n","def split_pool(pool, n_train, n_test, seed=999):\n","    set_seed(seed)\n","    idx = torch.randperm(len(pool)).tolist()\n","    train = [pool[i] for i in idx[:n_train]]\n","    test  = [pool[i] for i in idx[n_train:n_train+n_test]]\n","    return train, test\n","\n","F_pos_tr, F_pos_te = split_pool(F_pos_all, CFG[\"POS_TRAIN\"], CFG[\"POS_TEST\"])\n","F_neg_tr, F_neg_te = split_pool(F_neg_all, CFG[\"NEG_TRAIN\"], CFG[\"NEG_TEST\"])\n","print(f\"F+ train/test: {len(F_pos_tr)}/{len(F_pos_te)} | F- train/test: {len(F_neg_tr)}/{len(F_neg_te)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3vxXi4hQbz8","executionInfo":{"status":"ok","timestamp":1756172716441,"user_tz":-360,"elapsed":6926203,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"a9372393-5f3c-4164-912c-2429cb81ab10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated 30 positive models\n","Epoch 000 | loss 3.7323 | val 0.935\n","Epoch 040 | loss 3.6254 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.880\n","Epoch 000 | loss 4.0393 | val 0.934\n","Epoch 040 | loss 3.5837 | val 0.936\n","✅ Final (best-val) | val 0.936 | test 0.877\n","Epoch 000 | loss 3.8824 | val 0.935\n","Epoch 040 | loss 3.6192 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.879\n","Epoch 000 | loss 4.0750 | val 0.935\n","Epoch 040 | loss 3.6228 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.879\n","Epoch 000 | loss 4.0343 | val 0.935\n","Epoch 040 | loss 3.6133 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.880\n","Epoch 000 | loss 4.2224 | val 0.935\n","Epoch 040 | loss 3.6490 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.880\n","Epoch 000 | loss 3.9638 | val 0.934\n","Epoch 040 | loss 3.6025 | val 0.935\n","✅ Final (best-val) | val 0.936 | test 0.877\n","Epoch 000 | loss 3.8467 | val 0.935\n","Epoch 040 | loss 3.6223 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.881\n","Epoch 000 | loss 3.8810 | val 0.935\n","Epoch 040 | loss 3.5892 | val 0.935\n","✅ Final (best-val) | val 0.936 | test 0.876\n","Epoch 000 | loss 3.8431 | val 0.935\n","Epoch 040 | loss 3.6278 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.878\n","Epoch 000 | loss 3.8199 | val 0.935\n","Epoch 040 | loss 3.6376 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.880\n","Epoch 000 | loss 3.8336 | val 0.935\n","Epoch 040 | loss 3.6330 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.878\n","Epoch 000 | loss 3.9537 | val 0.935\n","Epoch 040 | loss 3.6146 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.879\n","Epoch 000 | loss 3.9255 | val 0.935\n","Epoch 040 | loss 3.6322 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.885\n","Epoch 000 | loss 3.7985 | val 0.935\n","Epoch 040 | loss 3.6074 | val 0.935\n","✅ Final (best-val) | val 0.936 | test 0.877\n","Epoch 000 | loss 4.0418 | val 0.935\n","Epoch 040 | loss 3.6399 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.876\n","Epoch 000 | loss 3.9202 | val 0.935\n","Epoch 040 | loss 3.6283 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.883\n","Epoch 000 | loss 3.9802 | val 0.935\n","Epoch 040 | loss 3.6216 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.878\n","Epoch 000 | loss 4.1272 | val 0.935\n","Epoch 040 | loss 3.5560 | val 0.936\n","✅ Final (best-val) | val 0.936 | test 0.875\n","Epoch 000 | loss 3.7030 | val 0.935\n","Epoch 040 | loss 3.6238 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.880\n","Epoch 000 | loss 3.7094 | val 0.935\n","Epoch 040 | loss 3.6175 | val 0.934\n","✅ Final (best-val) | val 0.935 | test 0.880\n","Epoch 000 | loss 4.0753 | val 0.935\n","Epoch 040 | loss 3.6474 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.878\n","Epoch 000 | loss 3.7807 | val 0.935\n","Epoch 040 | loss 3.6408 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.879\n","Epoch 000 | loss 3.7504 | val 0.935\n","Epoch 040 | loss 3.6249 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.879\n","Epoch 000 | loss 4.0708 | val 0.935\n","Epoch 040 | loss 3.6480 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.881\n","Epoch 000 | loss 3.9471 | val 0.935\n","Epoch 040 | loss 3.6231 | val 0.935\n","✅ Final (best-val) | val 0.936 | test 0.876\n","Epoch 000 | loss 3.8166 | val 0.935\n","Epoch 040 | loss 3.6322 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.878\n","Epoch 000 | loss 3.7880 | val 0.935\n","Epoch 040 | loss 3.6233 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.884\n","Epoch 000 | loss 3.8144 | val 0.935\n","Epoch 040 | loss 3.6282 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.881\n","Epoch 000 | loss 4.1370 | val 0.935\n","Epoch 040 | loss 3.6184 | val 0.935\n","✅ Final (best-val) | val 0.935 | test 0.879\n","Generated 30 negative models\n","F+ train/test: 15/15 | F- train/test: 15/15\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Fingerprint set for graph-level matching\n","# ================================\n","class FingerprintGraphPair(nn.Module):\n","    def __init__(self, n_nodes, feat_dim, edge_init_p=0.1):\n","        super().__init__()\n","        self.n = n_nodes\n","        self.d = feat_dim\n","\n","        # Graph 1\n","        X1 = torch.empty(self.n, self.d).uniform_(-0.5, 0.5)\n","        self.X1 = nn.Parameter(X1.to(device))\n","        A1_0 = (torch.rand(self.n, self.n, device=device) < edge_init_p).float()\n","        A1_0.fill_diagonal_(0.0)\n","        A1_0 = torch.maximum(A1_0, A1_0.T)\n","        self.A1_logits = nn.Parameter(torch.logit(torch.clamp(A1_0, 1e-4, 1-1e-4)))\n","\n","        # Graph 2\n","        X2 = torch.empty(self.n, self.d).uniform_(-0.5, 0.5)\n","        self.X2 = nn.Parameter(X2.to(device))\n","        A2_0 = (torch.rand(self.n, self.n, device=device) < edge_init_p).float()\n","        A2_0.fill_diagonal_(0.0)\n","        A2_0 = torch.maximum(A2_0, A2_0.T)\n","        self.A2_logits = nn.Parameter(torch.logit(torch.clamp(A2_0, 1e-4, 1-1e-4)))\n","\n","    @torch.no_grad()\n","    def edge_index_pair(self):\n","        # Graph 1\n","        A1_prob = torch.sigmoid(self.A1_logits)\n","        A1_bin = (A1_prob > 0.5).float()\n","        A1_bin.fill_diagonal_(0.0)\n","        A1_bin = torch.maximum(A1_bin, A1_bin.T)\n","        idx1 = A1_bin.nonzero(as_tuple=False)\n","        if idx1.numel() == 0:\n","            ei1 = torch.empty(2, 0, dtype=torch.long, device=device)\n","        else:\n","            ei1 = idx1.t().contiguous()\n","\n","        # Graph 2\n","        A2_prob = torch.sigmoid(self.A2_logits)\n","        A2_bin = (A2_prob > 0.5).float()\n","        A2_bin.fill_diagonal_(0.0)\n","        A2_bin = torch.maximum(A2_bin, A2_bin.T)\n","        idx2 = A2_bin.nonzero(as_tuple=False)\n","        if idx2.numel() == 0:\n","            ei2 = torch.empty(2, 0, dtype=torch.long, device=device)\n","        else:\n","            ei2 = idx2.t().contiguous()\n","\n","        return ei1, ei2\n","\n","    @torch.no_grad()\n","    def flip_topk_by_grad(self, gradA1, gradA2, topk=32, step=2.0):\n","        # Process Graph 1\n","        g1 = gradA1.abs()\n","        triu1 = torch.triu(torch.ones_like(g1), diagonal=1)\n","        scores1 = (g1 * triu1).flatten()\n","        k1 = min(topk, scores1.numel())\n","        if k1 > 0:\n","            _, idxs1 = torch.topk(scores1, k=k1)\n","            r1 = self.n\n","            pairs1 = torch.stack((idxs1 // r1, idxs1 % r1), dim=1)\n","            A1_prob = torch.sigmoid(self.A1_logits).detach()\n","            for (u, v) in pairs1.tolist():\n","                guv = gradA1[u, v].item()\n","                exist = A1_prob[u, v] > 0.5\n","                if exist and guv <= 0:\n","                    self.A1_logits.data[u, v] -= step\n","                    self.A1_logits.data[v, u] -= step\n","                elif (not exist) and guv >= 0:\n","                    self.A1_logits.data[u, v] += step\n","                    self.A1_logits.data[v, u] += step\n","            self.A1_logits.data.fill_diagonal_(-10.0)\n","\n","        # Process Graph 2 (similar logic)\n","        g2 = gradA2.abs()\n","        triu2 = torch.triu(torch.ones_like(g2), diagonal=1)\n","        scores2 = (g2 * triu2).flatten()\n","        k2 = min(topk, scores2.numel())\n","        if k2 > 0:\n","            _, idxs2 = torch.topk(scores2, k=k2)\n","            r2 = self.n\n","            pairs2 = torch.stack((idxs2 // r2, idxs2 % r2), dim=1)\n","            A2_prob = torch.sigmoid(self.A2_logits).detach()\n","            for (u, v) in pairs2.tolist():\n","                guv = gradA2[u, v].item()\n","                exist = A2_prob[u, v] > 0.5\n","                if exist and guv <= 0:\n","                    self.A2_logits.data[u, v] -= step\n","                    self.A2_logits.data[v, u] -= step\n","                elif (not exist) and guv >= 0:\n","                    self.A2_logits.data[u, v] += step\n","                    self.A2_logits.data[v, u] += step\n","            self.A2_logits.data.fill_diagonal_(-10.0)\n","\n","class FingerprintSet(nn.Module):\n","    def __init__(self, P, n_nodes, feat_dim, edge_init_p=0.1, topk_edges=32, edge_step=2.0):\n","        super().__init__()\n","        self.P = P\n","        self.fps = nn.ModuleList([\n","            FingerprintGraphPair(n_nodes, feat_dim, edge_init_p)\n","            for _ in range(P)\n","        ]).to(device)\n","        self.topk_edges = topk_edges\n","        self.edge_step = edge_step\n","\n","    def concat_outputs(self, model, *, require_grad: bool = False):\n","        outs = []\n","        model.eval()\n","        ctx = torch.enable_grad() if require_grad else torch.no_grad()\n","        with ctx:\n","            for fp in self.fps:\n","                ei1, ei2 = fp.edge_index_pair()\n","\n","                # Create batch indices for single graphs\n","                batch1 = torch.zeros(fp.n, dtype=torch.long, device=device)\n","                batch2 = torch.zeros(fp.n, dtype=torch.long, device=device)\n","\n","                # Get similarity score from model\n","                similarity = model(fp.X1, ei1, batch1, fp.X2, ei2, batch2)\n","                outs.append(similarity.unsqueeze(0))  # Ensure it's a tensor\n","        return torch.cat(outs, dim=0)\n","\n","    def flip_adj_by_grad(self, surrogate_grad_list):\n","        for fp, (g1, g2) in zip(self.fps, surrogate_grad_list):\n","            fp.flip_topk_by_grad(g1, g2, topk=self.topk_edges, step=self.edge_step)\n","\n","fp_set = FingerprintSet(\n","    P=CFG[\"FP_P\"],\n","    n_nodes=CFG[\"FP_NODES\"],\n","    feat_dim=num_feats,\n","    edge_init_p=CFG[\"FP_EDGE_INIT_P\"],\n","    topk_edges=CFG[\"FP_EDGE_TOPK\"],\n","    edge_step=CFG[\"EDGE_LOGIT_STEP\"],\n",")\n","\n","INPUT_DIM = CFG[\"FP_P\"]  # P graph pairs * 1 similarity score each\n","print(\"Univerifier input dim =\", INPUT_DIM)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufkEn1QMcTYs","executionInfo":{"status":"ok","timestamp":1756172716454,"user_tz":-360,"elapsed":36,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"907aae07-f0cd-49e7-875d-1c181b26481f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Univerifier input dim = 32\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Univerifier (binary classifier)\n","# ================================\n","class Univerifier(nn.Module):\n","    def __init__(self, input_dim: int):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 128), nn.LeakyReLU(0.01),\n","            nn.Linear(128, 64),        nn.LeakyReLU(0.01),\n","            nn.Linear(64, 32),         nn.LeakyReLU(0.01),\n","            nn.Linear(32, 2),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","V = Univerifier(INPUT_DIM).to(device)\n","opt_V = Adam(V.parameters(), lr=CFG[\"LR_V\"])\n"],"metadata":{"id":"MbtHtEsFcX0e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================\n","#  Joint learning for graph matching\n","# ================================\n","\n","models_pos_tr = [model_f.to(device)] + [m.to(device) for m in F_pos_tr]\n","models_neg_tr = [m.to(device) for m in F_neg_tr]\n","print(f\"Train pools -> Pos: {len(models_pos_tr)} | Neg: {len(models_neg_tr)}\")\n","\n","def batch_from_pool(fp_set, pos_models, neg_models, *, require_grad: bool):\n","    X = []; y = []\n","    for m in pos_models:\n","        X.append(fp_set.concat_outputs(m, require_grad=require_grad)); y.append(1)\n","    for m in neg_models:\n","        X.append(fp_set.concat_outputs(m, require_grad=require_grad)); y.append(0)\n","    return torch.stack(X, dim=0), torch.tensor(y, device=device)\n","\n","def surrogate_grad_A_for_fp_pair(fp, model):\n","    # Compute surrogate gradient for adjacency matrix of both graphs in the pair.\n","    with torch.no_grad():\n","        # Graph 1\n","        ei1, _ = fp.edge_index_pair()\n","        h1 = model.conv1(fp.X1, ei1)\n","        h1 = F.relu(h1)\n","        hn1 = F.normalize(h1, dim=-1)\n","        sim1 = hn1 @ hn1.t()\n","        gradA1 = sim1 - 0.5\n","\n","        # Graph 2\n","        _, ei2 = fp.edge_index_pair()\n","        h2 = model.conv1(fp.X2, ei2)\n","        h2 = F.relu(h2)\n","        hn2 = F.normalize(h2, dim=-1)\n","        sim2 = hn2 @ hn2.t()\n","        gradA2 = sim2 - 0.5\n","\n","    return gradA1.detach().cpu(), gradA2.detach().cpu()\n","\n","def update_features(fp_set, V, pos_models, neg_models, steps, lr_x):\n","    # Freeze model params\n","    for m in pos_models + neg_models:\n","        for p in m.parameters(): p.requires_grad_(False)\n","    # Turn on grad for X before building batch\n","    for fp in fp_set.fps:\n","        fp.X1.requires_grad_(True)\n","        fp.X2.requires_grad_(True)\n","\n","    for _ in range(steps):\n","        Xb, yb = batch_from_pool(fp_set, pos_models, neg_models, require_grad=True)\n","        V.eval()\n","        for p in V.parameters(): p.requires_grad_(False)\n","\n","        logits = V(Xb.to(device))\n","        loss = F.cross_entropy(logits, yb)\n","\n","        # Zero gradients for fingerprint features\n","        for fp in fp_set.fps:\n","            if fp.X1.grad is not None: fp.X1.grad.zero_()\n","            if fp.X2.grad is not None: fp.X2.grad.zero_()\n","\n","        loss.backward()\n","\n","        # Update fingerprint features with gradient\n","        with torch.no_grad():\n","            for fp in fp_set.fps:\n","                if fp.X1.grad is not None:\n","                    fp.X1.add_(lr_x * fp.X1.grad)\n","                    fp.X1.grad.zero_()\n","                if fp.X2.grad is not None:\n","                    fp.X2.add_(lr_x * fp.X2.grad)\n","                    fp.X2.grad.zero_()\n","\n","        for p in V.parameters(): p.requires_grad_(True)\n","\n","    # Get surrogate gradients for adjacency matrices\n","    grads = []\n","    for fp in fp_set.fps:\n","        gradA1, gradA2 = surrogate_grad_A_for_fp_pair(fp, pos_models[0])\n","        grads.append((gradA1, gradA2))\n","    fp_set.flip_adj_by_grad(grads)\n","\n","def update_verifier(fp_set, V, pos_models, neg_models, steps):\n","    for _ in range(steps):\n","        V.train()\n","        Xb, yb = batch_from_pool(fp_set, pos_models, neg_models, require_grad=False)\n","        logits = V(Xb.to(device))\n","        loss = F.cross_entropy(logits, yb)\n","        opt_V.zero_grad(); loss.backward(); opt_V.step()\n","\n","# ===== CRITICAL FIX: Add class balancing weights =====\n","# Calculate class weights to handle imbalance\n","pos_weight = torch.tensor([len(models_neg_tr) / len(models_pos_tr)], device=device)\n","neg_weight = torch.tensor([len(models_pos_tr) / len(models_neg_tr)], device=device)\n","class_weights = torch.cat([pos_weight, neg_weight])\n","\n","# Modify the update_verifier function to use weighted loss\n","def update_verifier_balanced(fp_set, V, pos_models, neg_models, steps):\n","    for _ in range(steps):\n","        V.train()\n","        Xb, yb = batch_from_pool(fp_set, pos_models, neg_models, require_grad=False)\n","        logits = V(Xb.to(device))\n","\n","        # Use weighted cross entropy to handle class imbalance\n","        loss = F.cross_entropy(logits, yb, weight=class_weights)\n","\n","        opt_V.zero_grad(); loss.backward(); opt_V.step()\n","\n","# ===== DEBUG: Check initial fingerprint outputs =====\n","print(\"🔍 Checking initial fingerprint outputs:\")\n","with torch.no_grad():\n","    # Check target model\n","    target_output = fp_set.concat_outputs(model_f, require_grad=False)\n","    print(f\"Target model output shape: {target_output.shape}\")\n","    print(f\"Target model outputs: {target_output.cpu().numpy()[:5]}\")  # First 5 values\n","\n","    # Check a positive model\n","    if models_pos_tr:\n","        pos_output = fp_set.concat_outputs(models_pos_tr[1], require_grad=False)  # Skip target model\n","        print(f\"Positive model outputs: {pos_output.cpu().numpy()[:5]}\")\n","\n","    # Check a negative model\n","    if models_neg_tr:\n","        neg_output = fp_set.concat_outputs(models_neg_tr[0], require_grad=False)\n","        print(f\"Negative model outputs: {neg_output.cpu().numpy()[:5]}\")\n","\n","# ===== Main joint learning loop =====\n","for it in range(1, CFG[\"OUTER_ITERS\"] + 1):\n","    update_features(fp_set, V, models_pos_tr, models_neg_tr, steps=CFG[\"FP_STEPS\"], lr_x=CFG[\"LR_X\"])\n","    update_verifier_balanced(fp_set, V, models_pos_tr, models_neg_tr, steps=CFG[\"V_STEPS\"])\n","\n","    V.eval()\n","    Xb, yb = batch_from_pool(fp_set, models_pos_tr, models_neg_tr, require_grad=False)\n","    with torch.no_grad():\n","        pred = V(Xb).argmax(dim=1)\n","        acc = (pred.cpu() == yb.cpu()).float().mean().item()\n","        pos_acc = (pred[:len(models_pos_tr)].cpu() == 1).float().mean().item()\n","        neg_acc = (pred[len(models_pos_tr):].cpu() == 0).float().mean().item()\n","\n","    print(f\"Iter {it:02d}/{CFG['OUTER_ITERS']} | train all {acc:.3f} | pos {pos_acc:.3f} | neg {neg_acc:.3f}\")\n","\n","    # Early stopping if negative accuracy improves\n","    if it > 2 and neg_acc > 0.7:  # If we get decent negative accuracy\n","        print(\"✅ Good negative accuracy achieved, continuing...\")\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnrhUnwVccEH","executionInfo":{"status":"ok","timestamp":1756173049006,"user_tz":-360,"elapsed":332499,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"98819426-6531-4da0-90f6-3f1737ba1d2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train pools -> Pos: 16 | Neg: 15\n","🔍 Checking initial fingerprint outputs:\n","Target model output shape: torch.Size([32])\n","Target model outputs: [0.6139181  0.61447287 0.6138921  0.6148934  0.6143709 ]\n","Positive model outputs: [0.60136217 0.601762   0.6013343  0.60200983 0.60168904]\n","Negative model outputs: [0.4084122  0.40825936 0.40966272 0.409209   0.40824747]\n","Iter 01/15 | train all 0.516 | pos 1.000 | neg 0.000\n","Iter 02/15 | train all 0.516 | pos 1.000 | neg 0.000\n","Iter 03/15 | train all 0.516 | pos 1.000 | neg 0.000\n","Iter 04/15 | train all 0.516 | pos 1.000 | neg 0.000\n","Iter 05/15 | train all 0.516 | pos 1.000 | neg 0.000\n","Iter 06/15 | train all 0.516 | pos 1.000 | neg 0.000\n","Iter 07/15 | train all 0.516 | pos 1.000 | neg 0.000\n","Iter 08/15 | train all 0.516 | pos 1.000 | neg 0.000\n","Iter 09/15 | train all 0.516 | pos 1.000 | neg 0.000\n","Iter 10/15 | train all 0.516 | pos 1.000 | neg 0.000\n","Iter 11/15 | train all 0.516 | pos 1.000 | neg 0.000\n","Iter 12/15 | train all 0.516 | pos 1.000 | neg 0.000\n","Iter 13/15 | train all 0.516 | pos 1.000 | neg 0.000\n","Iter 14/15 | train all 0.581 | pos 1.000 | neg 0.133\n","Iter 15/15 | train all 0.645 | pos 1.000 | neg 0.267\n"]}]},{"cell_type":"code","source":["# ===== DEBUG: Check initial fingerprint outputs =====\n","print(\"🔍 Checking initial fingerprint outputs:\")\n","with torch.no_grad():\n","    # Check target model\n","    target_output = fp_set.concat_outputs(model_f, require_grad=False)\n","    print(f\"Target model output shape: {target_output.shape}\")\n","    print(f\"Target model outputs: {target_output.cpu().numpy()[:5]}\")  # First 5 values\n","\n","    # Check a positive model\n","    if models_pos_tr:\n","        pos_output = fp_set.concat_outputs(models_pos_tr[1], require_grad=False)  # Skip target model\n","        print(f\"Positive model outputs: {pos_output.cpu().numpy()[:5]}\")\n","\n","    # Check a negative model\n","    if models_neg_tr:\n","        neg_output = fp_set.concat_outputs(models_neg_tr[0], require_grad=False)\n","        print(f\"Negative model outputs: {neg_output.cpu().numpy()[:5]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lO9JgMb5oTz4","executionInfo":{"status":"ok","timestamp":1756173049289,"user_tz":-360,"elapsed":285,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"f1ec6baa-c7f3-42b8-9530-49cd5b969425"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["🔍 Checking initial fingerprint outputs:\n","Target model output shape: torch.Size([32])\n","Target model outputs: [0.61382014 0.6145306  0.61382014 0.61477584 0.6143188 ]\n","Positive model outputs: [0.60130006 0.6018021  0.60129255 0.60192764 0.6016611 ]\n","Negative model outputs: [0.40871927 0.40810373 0.4099434  0.4094292  0.40833682]\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Held-out verification (Robustness/Uniqueness/ARUC)\n","# ================================\n","models_pos_te = [model_f.to(device)] + [m.to(device) for m in F_pos_te]\n","models_neg_te = [m.to(device) for m in F_neg_te]\n","\n","@torch.no_grad()\n","def verify_scores(V, fp_set, models):\n","    Xs = [fp_set.concat_outputs(m, require_grad=False) for m in models]\n","    logits = V(torch.stack(Xs, dim=0).to(device))\n","    probs = F.softmax(logits, dim=-1)[:, 1]  # p(positive)\n","    return probs.detach().cpu().numpy()\n","\n","def sweep_threshold(p_pos, p_neg, num=301):\n","    ths = np.linspace(0.0, 1.0, num=num)\n","    R = []  # Robustness (True Positive Rate)\n","    U = []  # Uniqueness (True Negative Rate)\n","    A = []  # Balanced Accuracy\n","\n","    for t in ths:\n","        tp = (p_pos >= t).mean()    # robustness\n","        tn = (p_neg <  t).mean()    # uniqueness\n","        R.append(tp)\n","        U.append(tn)\n","        A.append((tp + tn) / 2.0)   # balanced acc\n","\n","    return ths, np.array(R), np.array(U), np.array(A)\n","\n","# Get verification scores\n","p_pos = verify_scores(V, fp_set, models_pos_te)\n","p_neg = verify_scores(V, fp_set, models_neg_te)\n","\n","# Sweep thresholds and compute metrics\n","ths, R, U, A = sweep_threshold(p_pos, p_neg, num=301)\n","best_idx = A.argmax()\n","mean_acc = A.mean()\n","\n","# Compute ARUC (Area Under Robustness-Uniqueness Curve)\n","ARUC = np.trapezoid(np.minimum(R, U), ths) if hasattr(np, \"trapezoid\") else np.trapz(np.minimum(R, U), ths)\n","\n","print(f\"Best @ λ={ths[best_idx]:.3f} | Robustness={R[best_idx]:.3f} | Uniqueness={U[best_idx]:.3f} | MeanAcc*={A[best_idx]:.3f}\")\n","print(f\"Mean Test Accuracy (avg over λ): {mean_acc:.3f}\")\n","print(f\"ARUC (approx): {ARUC:.3f}\")\n","\n","# ================================\n","# Additional Analysis: Show some example predictions\n","# ================================\n","print(\"\\n=== Example Verification Scores ===\")\n","print(\"Positive models (should be high):\")\n","for i, score in enumerate(p_pos[:5]):\n","    print(f\"  Pos model {i+1}: {score:.3f}\")\n","\n","print(\"\\nNegative models (should be low):\")\n","for i, score in enumerate(p_neg[:5]):\n","    print(f\"  Neg model {i+1}: {score:.3f}\")\n","\n","print(f\"\\n✅ AIDS Graph Matching GNNFingers Implementation Complete!\")\n","print(f\"📊 Results Summary:\")\n","print(f\"   - ARUC: {ARUC:.3f}\")\n","print(f\"   - Best Robustness: {R[best_idx]:.3f}\")\n","print(f\"   - Best Uniqueness: {U[best_idx]:.3f}\")\n","print(f\"   - Mean Accuracy: {mean_acc:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jyVwCnHCxp00","executionInfo":{"status":"ok","timestamp":1756173053318,"user_tz":-360,"elapsed":4021,"user":{"displayName":"Fake Id","userId":"08296860444418316006"}},"outputId":"ebb818e3-7066-48dc-a93e-617ca7e91406"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best @ λ=0.553 | Robustness=1.000 | Uniqueness=1.000 | MeanAcc*=1.000\n","Mean Test Accuracy (avg over λ): 0.597\n","ARUC (approx): 0.195\n","\n","=== Example Verification Scores ===\n","Positive models (should be high):\n","  Pos model 1: 0.725\n","  Pos model 2: 0.711\n","  Pos model 3: 0.711\n","  Pos model 4: 0.715\n","  Pos model 5: 0.714\n","\n","Negative models (should be low):\n","  Neg model 1: 0.494\n","  Neg model 2: 0.525\n","  Neg model 3: 0.525\n","  Neg model 4: 0.528\n","  Neg model 5: 0.528\n","\n","✅ AIDS Graph Matching GNNFingers Implementation Complete!\n","📊 Results Summary:\n","   - ARUC: 0.195\n","   - Best Robustness: 1.000\n","   - Best Uniqueness: 1.000\n","   - Mean Accuracy: 0.597\n"]}]}]}