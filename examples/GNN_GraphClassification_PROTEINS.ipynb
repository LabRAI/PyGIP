{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ================================\n","# Environment Setup (Colab)\n","# ================================\n","\"\"\"\n","Install PyTorch and PyTorch Geometric for graph neural networks\n","Follow the paper's requirement for PyTorch and torch-geometric\n","\"\"\"\n","!pip -q install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n","!pip -q install torch-geometric torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n","print(\"✅ PyTorch & PyG installation completed\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SY0TzIBdpNS_","outputId":"679cefb9-1b41-4471-9ed5-7fb74721d4d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h✅ PyTorch & PyG installation completed\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Imports & Core Utilities\n","# ================================\n","\"\"\"\n","Import all necessary libraries and set up reproducible random seeds\n","Following the paper's experimental setup for consistent results\n","\"\"\"\n","import os, random, copy\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","\n","from torch_geometric.datasets import TUDataset\n","from torch_geometric.nn import GCNConv, SAGEConv, global_mean_pool, global_add_pool\n","from torch_geometric.loader import DataLoader\n","from torch_geometric.utils import to_undirected, subgraph\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Device: {device}')\n","\n","def set_seed(seed: int = 42):\n","    \"\"\"Set random seeds for reproducibility across all libraries\"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WqiGa620pXVI","outputId":"63837632-42a5-4f83-9b10-5889e202504d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.12/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n","  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /usr/local/lib/python3.12/dist-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n","  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.12/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n","  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"]},{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Dataset Loading & Splitting\n","# ================================\n","\"\"\"\n","Load PROTEINS dataset and create 7/1/2 train/val/test split\n","as specified in the paper (Section 4.1.1)\n","\"\"\"\n","# Load PROTEINS dataset from TUDataset\n","dataset = TUDataset(root='/content/data/PROTEINS', name='PROTEINS')\n","print(f\"Dataset: {dataset}\")\n","print(f\"Number of graphs: {len(dataset)}\")\n","print(f\"Number of features: {dataset.num_features}\")\n","print(f\"Number of classes: {dataset.num_classes}\")\n","\n","def make_graph_split(dataset, train_ratio=0.7, val_ratio=0.1, seed=1):\n","    \"\"\"\n","    Split graph dataset into train/validation/test sets\n","    Paper uses 7/1/2 split as mentioned in Section 4.1.1\n","    \"\"\"\n","    g = torch.Generator().manual_seed(seed)\n","    idx = torch.randperm(len(dataset), generator=g)\n","    n = len(dataset)\n","    n_tr = int(n * train_ratio)\n","    n_va = int(n * val_ratio)\n","\n","    train_idx = idx[:n_tr]\n","    val_idx = idx[n_tr:n_tr + n_va]\n","    test_idx = idx[n_tr + n_va:]\n","\n","    train_dataset = [dataset[i] for i in train_idx]\n","    val_dataset = [dataset[i] for i in val_idx]\n","    test_dataset = [dataset[i] for i in test_idx]\n","\n","    return train_dataset, val_dataset, test_dataset\n","\n","# Create dataset splits\n","train_dataset, val_dataset, test_dataset = make_graph_split(dataset, 0.7, 0.1, seed=1)\n","\n","# Create data loaders with appropriate batch size\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","num_feats = dataset.num_features\n","num_classes = dataset.num_classes\n","print(f'Features: {num_feats} | Classes: {num_classes}')\n","print(f'Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-gcZhRunpbhW","outputId":"d7075d46-a914-4c5d-ecc6-97a6131e5b7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip\n","Processing...\n","Done!\n"]},{"output_type":"stream","name":"stdout","text":["Dataset: PROTEINS(1113)\n","Number of graphs: 1113\n","Number of features: 3\n","Number of classes: 2\n","Features: 3 | Classes: 2\n","Train: 779 | Val: 111 | Test: 223\n"]}]},{"cell_type":"code","source":["# ================================\n","# Configuration Parameters\n","# ================================\n","\"\"\"\n","Configuration following the paper's experimental setup\n","All hyperparameters are based on Section 4.1 and appendix\n","\"\"\"\n","CFG = dict(\n","    # Model pool sizes (Section 4.1.2)\n","    POS_TRAIN=50,        # Positive models for training\n","    POS_TEST=50,         # Positive models for testing\n","    NEG_TRAIN=50,        # Negative models for training\n","    NEG_TEST=50,         # Negative models for testing\n","\n","    # Obfuscation techniques (Section 2.2.2)\n","    USE_FT_LAST=True,    # Fine-tune last layer only\n","    USE_FT_ALL=True,     # Fine-tune all layers\n","    USE_PR_LAST=True,    # Partial retrain last layer\n","    USE_PR_ALL=True,     # Partial retrain all layers\n","    USE_DISTILL=True,    # Knowledge distillation\n","    DISTILL_STEPS=250,   # Distillation training steps\n","\n","    # Graph fingerprint parameters (Section 3.3)\n","    FP_P=64,             # Number of fingerprint graphs (P)\n","    FP_NODES=32,         # Nodes per fingerprint graph (n)\n","    FP_EDGE_INIT_P=0.05, # Initial edge probability (r)\n","    FP_EDGE_TOPK=96,     # Top-k edges to flip (K)\n","    EDGE_LOGIT_STEP=2.5, # Edge logit update step size\n","\n","    # Joint learning parameters (Section 3.4)\n","    OUTER_ITERS=20,      # Joint learning iterations\n","    FP_STEPS=5,          # Feature update steps per iteration\n","    V_STEPS=10,          # Verifier update steps per iteration\n","\n","    # Learning rates (Section 4.1.5)\n","    LR_TARGET=0.005,     # Target model learning rate\n","    WD_TARGET=5e-4,      # Target model weight decay\n","    LR_V=1e-3,           # Verifier learning rate\n","    LR_X=1e-3,           # Feature learning rate\n","\n","    SEED=1,              # Global random seed\n",")\n","print(\"Configuration:\", CFG)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMzZG9lEpgbo","outputId":"6bb95708-f767-4952-8447-0db099a60f61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Configuration: {'POS_TRAIN': 50, 'POS_TEST': 50, 'NEG_TRAIN': 50, 'NEG_TEST': 50, 'USE_FT_LAST': True, 'USE_FT_ALL': True, 'USE_PR_LAST': True, 'USE_PR_ALL': True, 'USE_DISTILL': True, 'DISTILL_STEPS': 250, 'FP_P': 64, 'FP_NODES': 32, 'FP_EDGE_INIT_P': 0.05, 'FP_EDGE_TOPK': 96, 'EDGE_LOGIT_STEP': 2.5, 'OUTER_ITERS': 20, 'FP_STEPS': 5, 'V_STEPS': 10, 'LR_TARGET': 0.005, 'WD_TARGET': 0.0005, 'LR_V': 0.001, 'LR_X': 0.001, 'SEED': 1}\n"]}]},{"cell_type":"code","source":["# ================================\n","#  GNN Model Architectures\n","# ================================\n","\"\"\"\n","Define 3-layer GNN models for graph classification\n","Following the paper's architecture specification (Section 4.1.5)\n","\"\"\"\n","class GCN_GraphCls(nn.Module):\n","    \"\"\"\n","    3-layer Graph Convolutional Network for graph classification\n","    Architecture: GCN -> ReLU -> Dropout -> GCN -> ReLU -> Dropout -> GCN -> Global Pool -> Linear\n","    \"\"\"\n","    def __init__(self, in_channels, hidden, out_channels, dropout=0.5):\n","        super().__init__()\n","        # 3-layer GCN as specified in the paper\n","        self.conv1 = GCNConv(in_channels, hidden)\n","        self.conv2 = GCNConv(hidden, hidden)\n","        self.conv3 = GCNConv(hidden, hidden)\n","        self.classifier = nn.Linear(hidden, out_channels)\n","        self.dropout = dropout\n","\n","    def forward(self, x, edge_index, batch):\n","        # First GCN layer\n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","\n","        # Second GCN layer\n","        x = self.conv2(x, edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","\n","        # Third GCN layer (no dropout after final conv)\n","        x = self.conv3(x, edge_index)\n","\n","        # Graph-level pooling (mean pooling as commonly used)\n","        x = global_mean_pool(x, batch)\n","\n","        # Classification layer\n","        x = self.classifier(x)\n","        return x\n","\n","class GraphSAGE_GraphCls(nn.Module):\n","    \"\"\"\n","    3-layer GraphSAGE for graph classification\n","    Used as student model in distillation experiments\n","    \"\"\"\n","    def __init__(self, in_channels, hidden, out_channels, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden)\n","        self.conv2 = SAGEConv(hidden, hidden)\n","        self.conv3 = SAGEConv(hidden, hidden)\n","        self.classifier = nn.Linear(hidden, out_channels)\n","        self.dropout = dropout\n","\n","    def forward(self, x, edge_index, batch):\n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","\n","        x = self.conv2(x, edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","\n","        x = self.conv3(x, edge_index)\n","\n","        # Graph-level pooling\n","        x = global_mean_pool(x, batch)\n","\n","        # Classification\n","        x = self.classifier(x)\n","        return x"],"metadata":{"id":"ru-s1A3upkxG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================\n","#  Training Utilities\n","# ================================\n","\"\"\"\n","Training and evaluation functions for graph classification models\n","Following standard practices with early stopping on validation\n","\"\"\"\n","@torch.no_grad()\n","def evaluate_graph_cls(model, loader):\n","    \"\"\"Evaluate graph classification model accuracy\"\"\"\n","    model.eval()\n","    total_correct = 0\n","    total_samples = 0\n","\n","    for batch in loader:\n","        batch = batch.to(device)\n","        out = model(batch.x, batch.edge_index, batch.batch)\n","        pred = out.argmax(dim=1)\n","        total_correct += (pred == batch.y).sum().item()\n","        total_samples += batch.y.size(0)\n","\n","    return total_correct / total_samples\n","\n","def train_graph_classifier(model, train_loader, val_loader, test_loader,\n","                          epochs=200, lr=0.005, wd=5e-4, verbose=True):\n","    \"\"\"\n","    Train graph classification model with early stopping\n","    Returns the best model based on validation accuracy\n","    \"\"\"\n","    model = model.to(device)\n","    opt = Adam(model.parameters(), lr=lr, weight_decay=wd)\n","\n","    best = {'val': 0.0, 'state': None}\n","\n","    for ep in range(epochs):\n","        model.train()\n","        total_loss = 0\n","\n","        # Training loop\n","        for batch in train_loader:\n","            batch = batch.to(device)\n","            opt.zero_grad()\n","            out = model(batch.x, batch.edge_index, batch.batch)\n","            loss = F.cross_entropy(out, batch.y)\n","            loss.backward()\n","            opt.step()\n","            total_loss += loss.item()\n","\n","        # Evaluation\n","        train_acc = evaluate_graph_cls(model, train_loader)\n","        val_acc = evaluate_graph_cls(model, val_loader)\n","        test_acc = evaluate_graph_cls(model, test_loader)\n","\n","        # Save best model based on validation accuracy\n","        if val_acc > best['val']:\n","            best['val'] = val_acc\n","            best['state'] = copy.deepcopy(model.state_dict())\n","\n","        if verbose and ep % 20 == 0:\n","            avg_loss = total_loss / len(train_loader)\n","            print(f\"Epoch {ep:03d} | loss {avg_loss:.4f} | train {train_acc:.3f} | val {val_acc:.3f} | test {test_acc:.3f}\")\n","\n","    # Load best model\n","    if best['state'] is not None:\n","        model.load_state_dict(best['state'])\n","\n","    # Final evaluation\n","    train_acc = evaluate_graph_cls(model, train_loader)\n","    val_acc = evaluate_graph_cls(model, val_loader)\n","    test_acc = evaluate_graph_cls(model, test_loader)\n","\n","    if verbose:\n","        print(f\"✅ Final (best-val) | train {train_acc:.3f} | val {val_acc:.3f} | test {test_acc:.3f}\")\n","\n","    return model"],"metadata":{"id":"WLReORWXppTx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================\n","#  Target Model Training\n","# ================================\n","\"\"\"\n","Train the target model (F) that we want to protect\n","This is the main GNN whose ownership we want to verify\n","\"\"\"\n","print(\"Training target model F (GCN for graph classification)...\")\n","set_seed(CFG[\"SEED\"])\n","model_f = GCN_GraphCls(num_feats, hidden=16, out_channels=num_classes, dropout=0.5)\n","model_f = train_graph_classifier(\n","    model_f, train_loader, val_loader, test_loader,\n","    epochs=200, lr=CFG[\"LR_TARGET\"], wd=CFG[\"WD_TARGET\"]\n",")\n","print(\"Target model training completed.\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lyFRmh_dpqeg","outputId":"e6bac169-3dbc-4828-eb6c-806b0bf10c33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training target model F (GCN for graph classification)...\n","Epoch 000 | loss 0.6716 | train 0.596 | val 0.631 | test 0.578\n","Epoch 020 | loss 0.6204 | train 0.716 | val 0.658 | test 0.668\n","Epoch 040 | loss 0.6035 | train 0.723 | val 0.667 | test 0.677\n","Epoch 060 | loss 0.6100 | train 0.736 | val 0.685 | test 0.673\n","Epoch 080 | loss 0.5943 | train 0.724 | val 0.703 | test 0.682\n","Epoch 100 | loss 0.5952 | train 0.727 | val 0.676 | test 0.673\n","Epoch 120 | loss 0.5978 | train 0.741 | val 0.703 | test 0.682\n","Epoch 140 | loss 0.6002 | train 0.707 | val 0.730 | test 0.673\n","Epoch 160 | loss 0.5917 | train 0.719 | val 0.748 | test 0.686\n","Epoch 180 | loss 0.5883 | train 0.742 | val 0.703 | test 0.677\n","✅ Final (best-val) | train 0.728 | val 0.766 | test 0.700\n","Target model training completed.\n","\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Suspect Model Generation - Utilities\n","# ================================\n","\"\"\"\n","Functions to create positive and negative suspect models\n","Positive models: Derived from target model via obfuscation techniques\n","Negative models: Independently trained models\n","\"\"\"\n","@torch.no_grad()\n","def reset_module(m):\n","    \"\"\"Reset module parameters to random initialization\"\"\"\n","    for layer in m.modules():\n","        if hasattr(layer, 'reset_parameters'):\n","            layer.reset_parameters()\n","\n","def ft_graph_model(base_model, train_loader, last_only=True, epochs=10, lr=0.005, seed=123):\n","    \"\"\"\n","    Fine-tuning obfuscation technique (Section 2.2.2)\n","    last_only: True = fine-tune only classifier, False = fine-tune all layers\n","    \"\"\"\n","    set_seed(seed)\n","    m = copy.deepcopy(base_model).to(device)\n","\n","    # Freeze parameters if last_only\n","    for p in m.parameters():\n","        p.requires_grad_(not last_only)\n","\n","    # Always fine-tune classifier (last layer)\n","    for p in m.classifier.parameters():\n","        p.requires_grad_(True)\n","\n","    opt = Adam(filter(lambda p: p.requires_grad, m.parameters()), lr=lr)\n","\n","    for _ in range(epochs):\n","        m.train()\n","        for batch in train_loader:\n","            batch = batch.to(device)\n","            opt.zero_grad()\n","            out = m(batch.x, batch.edge_index, batch.batch)\n","            loss = F.cross_entropy(out, batch.y)\n","            loss.backward()\n","            opt.step()\n","\n","    return m.eval()\n","\n","def pr_graph_model(base_model, train_loader, last_only=True, epochs=10, lr=0.005, seed=456):\n","    \"\"\"\n","    Partial retraining obfuscation technique (Section 2.2.2)\n","    Reset parameters and retrain selected layers\n","    \"\"\"\n","    set_seed(seed)\n","    m = copy.deepcopy(base_model).to(device)\n","\n","    # Reset parameters based on strategy\n","    if last_only:\n","        reset_module(m.classifier)\n","    else:\n","        reset_module(m)\n","\n","    opt = Adam(m.parameters(), lr=lr)\n","\n","    for _ in range(epochs):\n","        m.train()\n","        for batch in train_loader:\n","            batch = batch.to(device)\n","            opt.zero_grad()\n","            out = m(batch.x, batch.edge_index, batch.batch)\n","            loss = F.cross_entropy(out, batch.y)\n","            loss.backward()\n","            opt.step()\n","\n","    return m.eval()\n","\n","def make_graph_student(arch='GCN', hidden=16):\n","    \"\"\"Create student model for knowledge distillation\"\"\"\n","    if arch == 'GCN':\n","        return GCN_GraphCls(num_feats, hidden, num_classes, dropout=0.5).to(device)\n","    else:  # arch == 'SAGE'\n","        return GraphSAGE_GraphCls(num_feats, hidden, num_classes, dropout=0.5).to(device)\n","\n","def distill_from_graph_teacher(teacher, train_loader, arch='GCN', T=2.0, steps=250, lr=0.01, seed=777):\n","    \"\"\"\n","    Knowledge distillation obfuscation technique (Section 2.2.2)\n","    Train student model to mimic teacher model outputs\n","    \"\"\"\n","    set_seed(seed)\n","    student = make_graph_student(arch, hidden=16)\n","    opt = Adam(student.parameters(), lr=lr)\n","    kl = nn.KLDivLoss(reduction='batchmean')\n","\n","    teacher.eval()\n","    data_iter = iter(train_loader)\n","\n","    for step in range(steps):\n","        try:\n","            batch = next(data_iter)\n","        except StopIteration:\n","            data_iter = iter(train_loader)\n","            batch = next(data_iter)\n","\n","        batch = batch.to(device)\n","\n","        # Teacher predictions (soft targets)\n","        with torch.no_grad():\n","            teacher_logits = teacher(batch.x, batch.edge_index, batch.batch)\n","            p_t = F.softmax(teacher_logits / T, dim=-1)\n","\n","        # Student predictions\n","        student.train()\n","        opt.zero_grad()\n","        student_logits = student(batch.x, batch.edge_index, batch.batch)\n","        logit_s = student_logits / T\n","        loss = kl(F.log_softmax(logit_s, dim=-1), p_t) * (T * T)\n","        loss.backward()\n","        opt.step()\n","\n","        if step % 50 == 0:\n","            print(f\"  Distillation step {step}/{steps}, loss: {loss.item():.4f}\")\n","\n","    return student.eval()"],"metadata":{"id":"DTaS1Gr0pykX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Positive Model Generation (F+)\n","# ================================\n","\"\"\"\n","Create positive suspect models using various obfuscation techniques\n","These models should be recognized as pirated versions of the target model\n","\"\"\"\n","def _distribute_budget(total, keys):\n","    \"\"\"Distribute total budget evenly across keys with remainder handling\"\"\"\n","    if not keys:\n","        return {}\n","    base = total // len(keys)\n","    rem = total - base * len(keys)\n","    out = {k: base for k in keys}\n","    for k in keys[:rem]:\n","        out[k] += 1\n","    return out\n","\n","print(\"Creating positive suspect models (F+)...\")\n","\n","F_pos_all = []\n","pos_total = CFG[\"POS_TRAIN\"] + CFG[\"POS_TEST\"]\n","\n","# Determine which obfuscation techniques to use\n","pos_keys = []\n","if CFG[\"USE_FT_LAST\"]: pos_keys.append(\"FT_LAST\")\n","if CFG[\"USE_FT_ALL\"]:  pos_keys.append(\"FT_ALL\")\n","if CFG[\"USE_PR_LAST\"]: pos_keys.append(\"PR_LAST\")\n","if CFG[\"USE_PR_ALL\"]:  pos_keys.append(\"PR_ALL\")\n","if CFG[\"USE_DISTILL\"]: pos_keys.append(\"DISTILL\")\n","\n","pos_budget = _distribute_budget(pos_total, pos_keys)\n","print(f\"Positive budget distribution: {pos_budget}\")\n","\n","seed_base = 10\n","for key in pos_keys:\n","    cnt = pos_budget[key]\n","    print(f\"Creating {cnt} {key} models...\")\n","\n","    if key == \"FT_LAST\":\n","        # Fine-tune last layer only\n","        for s in range(seed_base, seed_base + cnt):\n","            model = ft_graph_model(model_f, train_loader, last_only=True, epochs=10, seed=s)\n","            F_pos_all.append(model)\n","        seed_base += cnt\n","\n","    elif key == \"FT_ALL\":\n","        # Fine-tune all layers\n","        for s in range(seed_base, seed_base + cnt):\n","            model = ft_graph_model(model_f, train_loader, last_only=False, epochs=10, seed=s)\n","            F_pos_all.append(model)\n","        seed_base += cnt\n","\n","    elif key == \"PR_LAST\":\n","        # Partial retrain last layer only\n","        for s in range(seed_base, seed_base + cnt):\n","            model = pr_graph_model(model_f, train_loader, last_only=True, epochs=10, seed=s)\n","            F_pos_all.append(model)\n","        seed_base += cnt\n","\n","    elif key == \"PR_ALL\":\n","        # Partial retrain all layers\n","        for s in range(seed_base, seed_base + cnt):\n","            model = pr_graph_model(model_f, train_loader, last_only=False, epochs=10, seed=s)\n","            F_pos_all.append(model)\n","        seed_base += cnt\n","\n","    elif key == \"DISTILL\":\n","        # Knowledge distillation with different architectures\n","        arches = (['GCN'] * (cnt//2) + ['SAGE'] * (cnt - cnt//2))\n","        for i, arch in enumerate(arches):\n","            print(f\"  Distilling to {arch} architecture...\")\n","            model = distill_from_graph_teacher(\n","                model_f, train_loader, arch=arch,\n","                T=2.0, steps=CFG[\"DISTILL_STEPS\"], seed=1000+i\n","            )\n","            F_pos_all.append(model)\n","\n","assert len(F_pos_all) == pos_total, f\"Expected {pos_total} positive models, got {len(F_pos_all)}\"\n","print(f\"Created {len(F_pos_all)} positive models\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQ10H-FQp3LP","outputId":"a95ffa62-4536-49ad-a69e-5c8f82f749ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating positive suspect models (F+)...\n","Positive budget distribution: {'FT_LAST': 20, 'FT_ALL': 20, 'PR_LAST': 20, 'PR_ALL': 20, 'DISTILL': 20}\n","Creating 20 FT_LAST models...\n","Creating 20 FT_ALL models...\n","Creating 20 PR_LAST models...\n","Creating 20 PR_ALL models...\n","Creating 20 DISTILL models...\n","  Distilling to GCN architecture...\n","  Distillation step 0/250, loss: 0.0916\n","  Distillation step 50/250, loss: 0.0206\n","  Distillation step 100/250, loss: 0.0122\n","  Distillation step 150/250, loss: 0.0145\n","  Distillation step 200/250, loss: 0.0141\n","  Distilling to GCN architecture...\n","  Distillation step 0/250, loss: 0.0732\n","  Distillation step 50/250, loss: 0.0159\n","  Distillation step 100/250, loss: 0.0162\n","  Distillation step 150/250, loss: 0.0080\n","  Distillation step 200/250, loss: 0.0099\n","  Distilling to GCN architecture...\n","  Distillation step 0/250, loss: 0.1428\n","  Distillation step 50/250, loss: 0.0363\n","  Distillation step 100/250, loss: 0.0216\n","  Distillation step 150/250, loss: 0.0230\n","  Distillation step 200/250, loss: 0.0180\n","  Distilling to GCN architecture...\n","  Distillation step 0/250, loss: 0.1276\n","  Distillation step 50/250, loss: 0.0234\n","  Distillation step 100/250, loss: 0.0197\n","  Distillation step 150/250, loss: 0.0230\n","  Distillation step 200/250, loss: 0.0199\n","  Distilling to GCN architecture...\n","  Distillation step 0/250, loss: 0.0604\n","  Distillation step 50/250, loss: 0.0291\n","  Distillation step 100/250, loss: 0.0159\n","  Distillation step 150/250, loss: 0.0077\n","  Distillation step 200/250, loss: 0.0153\n","  Distilling to GCN architecture...\n","  Distillation step 0/250, loss: 0.0962\n","  Distillation step 50/250, loss: 0.0182\n","  Distillation step 100/250, loss: 0.0090\n","  Distillation step 150/250, loss: 0.0077\n","  Distillation step 200/250, loss: 0.0150\n","  Distilling to GCN architecture...\n","  Distillation step 0/250, loss: 0.2025\n","  Distillation step 50/250, loss: 0.0316\n","  Distillation step 100/250, loss: 0.0172\n","  Distillation step 150/250, loss: 0.0089\n","  Distillation step 200/250, loss: 0.0202\n","  Distilling to GCN architecture...\n","  Distillation step 0/250, loss: 0.0639\n","  Distillation step 50/250, loss: 0.0164\n","  Distillation step 100/250, loss: 0.0061\n","  Distillation step 150/250, loss: 0.0080\n","  Distillation step 200/250, loss: 0.0066\n","  Distilling to GCN architecture...\n","  Distillation step 0/250, loss: 0.1437\n","  Distillation step 50/250, loss: 0.0446\n","  Distillation step 100/250, loss: 0.0179\n","  Distillation step 150/250, loss: 0.0184\n","  Distillation step 200/250, loss: 0.0150\n","  Distilling to GCN architecture...\n","  Distillation step 0/250, loss: 0.0720\n","  Distillation step 50/250, loss: 0.0178\n","  Distillation step 100/250, loss: 0.0116\n","  Distillation step 150/250, loss: 0.0091\n","  Distillation step 200/250, loss: 0.0115\n","  Distilling to SAGE architecture...\n","  Distillation step 0/250, loss: 0.0830\n","  Distillation step 50/250, loss: 0.0357\n","  Distillation step 100/250, loss: 0.0128\n","  Distillation step 150/250, loss: 0.0126\n","  Distillation step 200/250, loss: 0.0081\n","  Distilling to SAGE architecture...\n","  Distillation step 0/250, loss: 0.1596\n","  Distillation step 50/250, loss: 0.0423\n","  Distillation step 100/250, loss: 0.0364\n","  Distillation step 150/250, loss: 0.0137\n","  Distillation step 200/250, loss: 0.0122\n","  Distilling to SAGE architecture...\n","  Distillation step 0/250, loss: 0.1456\n","  Distillation step 50/250, loss: 0.0341\n","  Distillation step 100/250, loss: 0.0194\n","  Distillation step 150/250, loss: 0.0156\n","  Distillation step 200/250, loss: 0.0149\n","  Distilling to SAGE architecture...\n","  Distillation step 0/250, loss: 0.0796\n","  Distillation step 50/250, loss: 0.0324\n","  Distillation step 100/250, loss: 0.0212\n","  Distillation step 150/250, loss: 0.0138\n","  Distillation step 200/250, loss: 0.0176\n","  Distilling to SAGE architecture...\n","  Distillation step 0/250, loss: 0.1059\n","  Distillation step 50/250, loss: 0.0315\n","  Distillation step 100/250, loss: 0.0150\n","  Distillation step 150/250, loss: 0.0173\n","  Distillation step 200/250, loss: 0.0086\n","  Distilling to SAGE architecture...\n","  Distillation step 0/250, loss: 0.0896\n","  Distillation step 50/250, loss: 0.0244\n","  Distillation step 100/250, loss: 0.0133\n","  Distillation step 150/250, loss: 0.0229\n","  Distillation step 200/250, loss: 0.0119\n","  Distilling to SAGE architecture...\n","  Distillation step 0/250, loss: 0.1282\n","  Distillation step 50/250, loss: 0.0491\n","  Distillation step 100/250, loss: 0.0215\n","  Distillation step 150/250, loss: 0.0090\n","  Distillation step 200/250, loss: 0.0130\n","  Distilling to SAGE architecture...\n","  Distillation step 0/250, loss: 0.0706\n","  Distillation step 50/250, loss: 0.0169\n","  Distillation step 100/250, loss: 0.0180\n","  Distillation step 150/250, loss: 0.0142\n","  Distillation step 200/250, loss: 0.0079\n","  Distilling to SAGE architecture...\n","  Distillation step 0/250, loss: 0.0971\n","  Distillation step 50/250, loss: 0.0369\n","  Distillation step 100/250, loss: 0.0354\n","  Distillation step 150/250, loss: 0.0244\n","  Distillation step 200/250, loss: 0.0189\n","  Distilling to SAGE architecture...\n","  Distillation step 0/250, loss: 0.0979\n","  Distillation step 50/250, loss: 0.0383\n","  Distillation step 100/250, loss: 0.0148\n","  Distillation step 150/250, loss: 0.0229\n","  Distillation step 200/250, loss: 0.0117\n","Created 100 positive models\n","\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Negative Model Generation (F-)\n","# ================================\n","\"\"\"\n","Create negative suspect models (independently trained)\n","These models should NOT be recognized as pirated versions\n","\"\"\"\n","print(\"Creating negative suspect models (F-)...\")\n","\n","F_neg_all = []\n","neg_total = CFG[\"NEG_TRAIN\"] + CFG[\"NEG_TEST\"]\n","neg_keys = [\"GCN\", \"SAGE\"]\n","neg_budget = _distribute_budget(neg_total, neg_keys)\n","print(f\"Negative budget distribution: {neg_budget}\")\n","\n","seed_base = 500\n","\n","# Create independent GCN models\n","print(f\"Creating {neg_budget['GCN']} independent GCN models...\")\n","for s in range(seed_base, seed_base + neg_budget[\"GCN\"]):\n","    set_seed(s)\n","    m = GCN_GraphCls(num_feats, 16, num_classes, dropout=0.5)\n","    m = train_graph_classifier(\n","        m, train_loader, val_loader, test_loader,\n","        epochs=120, lr=CFG[\"LR_TARGET\"], wd=CFG[\"WD_TARGET\"], verbose=False\n","    )\n","    F_neg_all.append(m.eval())\n","\n","seed_base += neg_budget[\"GCN\"]\n","\n","# Create independent GraphSAGE models\n","print(f\"Creating {neg_budget['SAGE']} independent SAGE models...\")\n","for s in range(seed_base, seed_base + neg_budget[\"SAGE\"]):\n","    set_seed(s)\n","    m = GraphSAGE_GraphCls(num_feats, 32, num_classes, dropout=0.5)\n","    m = train_graph_classifier(\n","        m, train_loader, val_loader, test_loader,\n","        epochs=120, lr=CFG[\"LR_TARGET\"], wd=CFG[\"WD_TARGET\"], verbose=False\n","    )\n","    F_neg_all.append(m.eval())\n","\n","assert len(F_neg_all) == neg_total, f\"Expected {neg_total} negative models, got {len(F_neg_all)}\"\n","print(f\"Created {len(F_neg_all)} negative models\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lljVLoUrp9D-","outputId":"c798881d-fac4-4000-ba98-534dbc87280b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating negative suspect models (F-)...\n","Negative budget distribution: {'GCN': 50, 'SAGE': 50}\n","Creating 50 independent GCN models...\n","Creating 50 independent SAGE models...\n","Created 100 negative models\n","\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Model Pool Splitting\n","# ================================\n","\"\"\"\n","Split positive and negative models into training and testing pools\n","Training pools: Used for fingerprint optimization and verifier training\n","Testing pools: Used for final evaluation (held-out testing)\n","\"\"\"\n","def split_pool(pool, n_train, n_test, seed=999):\n","    \"\"\"Split model pool into train/test sets\"\"\"\n","    set_seed(seed)\n","    idx = torch.randperm(len(pool)).tolist()\n","    train = [pool[i] for i in idx[:n_train]]\n","    test  = [pool[i] for i in idx[n_train:n_train + n_test]]\n","    return train, test\n","\n","# Split positive and negative pools\n","F_pos_tr, F_pos_te = split_pool(F_pos_all, CFG[\"POS_TRAIN\"], CFG[\"POS_TEST\"])\n","F_neg_tr, F_neg_te = split_pool(F_neg_all, CFG[\"NEG_TRAIN\"], CFG[\"NEG_TEST\"])\n","\n","print(f\"Model pool split completed:\")\n","print(f\"F+ train/test: {len(F_pos_tr)}/{len(F_pos_te)}\")\n","print(f\"F- train/test: {len(F_neg_tr)}/{len(F_neg_te)}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zjb3g2oJqC2f","outputId":"11f7c58c-5730-4482-b882-e6be5c35cab9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model pool split completed:\n","F+ train/test: 50/50\n","F- train/test: 50/50\n","\n"]}]},{"cell_type":"code","source":["# ================================\n","# Graph Fingerprint Implementation\n","# ================================\n","\"\"\"\n","Graph fingerprint construction for graph-level tasks (Section 3.3)\n","Each fingerprint is a small graph with learnable features and adjacency matrix\n","\"\"\"\n","class GraphFingerprint(nn.Module):\n","    \"\"\"\n","    Individual graph fingerprint for graph-level tasks\n","    Maintains differentiable adjacency matrix via logits and learnable node features\n","    \"\"\"\n","    def __init__(self, n_nodes, feat_dim, edge_init_p=0.05):\n","        super().__init__()\n","        self.n = n_nodes\n","        self.d = feat_dim\n","\n","        # Initialize node features uniformly\n","        X = torch.empty(self.n, self.d).uniform_(-0.5, 0.5)\n","        self.X = nn.Parameter(X.to(device))\n","\n","        # Initialize adjacency matrix as logits for differentiability\n","        # Start with low edge probability as specified in paper\n","        A0 = (torch.rand(self.n, self.n, device=device) < edge_init_p).float()\n","        A0.fill_diagonal_(0.0)  # No self-loops\n","        A0 = torch.maximum(A0, A0.T)  # Make symmetric\n","\n","        # Convert to logits (avoiding numerical issues)\n","        self.A_logits = nn.Parameter(torch.logit(torch.clamp(A0, 1e-4, 1-1e-4)))\n","\n","    @torch.no_grad()\n","    def edge_index(self):\n","        \"\"\"\n","        Convert adjacency logits to edge_index format\n","        Used for GNN forward passes\n","        \"\"\"\n","        A_prob = torch.sigmoid(self.A_logits)\n","        A_bin = (A_prob > 0.5).float()\n","        A_bin.fill_diagonal_(0.0)\n","        A_bin = torch.maximum(A_bin, A_bin.T)  # Ensure symmetry\n","\n","        # Convert to edge_index format\n","        idx = A_bin.nonzero(as_tuple=False)\n","        if idx.numel() == 0:\n","            return torch.empty(2, 0, dtype=torch.long, device=device)\n","        return idx.t().contiguous()\n","\n","    @torch.no_grad()\n","    def flip_topk_by_grad(self, gradA, topk=64, step=2.5):\n","        \"\"\"\n","        Flip top-k edges based on gradient magnitude (Section 3.4.2)\n","        Following paper's discrete optimization strategy\n","        \"\"\"\n","        g = gradA.abs()\n","        # Only consider upper triangular part to avoid double-counting\n","        triu = torch.triu(torch.ones_like(g), diagonal=1)\n","        scores = (g * triu).flatten()\n","        k = min(topk, scores.numel())\n","        if k == 0:\n","            return\n","\n","        # Find top-k edges by gradient magnitude\n","        _, idxs = torch.topk(scores, k=k)\n","        r = self.n\n","        pairs = torch.stack((idxs // r, idxs % r), dim=1)\n","\n","        A_prob = torch.sigmoid(self.A_logits).detach()\n","\n","        # Apply edge flipping rules from paper\n","        for (u, v) in pairs.tolist():\n","            guv = gradA[u, v].item()\n","            exist = A_prob[u, v] > 0.5\n","\n","            if exist and guv <= 0:  # Remove existing edge\n","                self.A_logits.data[u, v] -= step\n","                self.A_logits.data[v, u] -= step\n","            elif (not exist) and guv >= 0:  # Add new edge\n","                self.A_logits.data[u, v] += step\n","                self.A_logits.data[v, u] += step\n","\n","        # Ensure no self-loops\n","        self.A_logits.data.fill_diagonal_(-10.0)\n","\n","class GraphFingerprintSet(nn.Module):\n","    \"\"\"\n","    Set of P graph fingerprints for graph-level tasks (Section 3.3)\n","    Manages multiple fingerprint graphs and their optimization\n","    \"\"\"\n","    def __init__(self, P, n_nodes, feat_dim, edge_init_p=0.05, topk_edges=64, edge_step=2.5):\n","        super().__init__()\n","        self.P = P\n","        self.fps = nn.ModuleList([\n","            GraphFingerprint(n_nodes, feat_dim, edge_init_p)\n","            for _ in range(P)\n","        ]).to(device)\n","        self.topk_edges = topk_edges\n","        self.edge_step = edge_step\n","\n","    def concat_outputs(self, model, *, require_grad: bool = False):\n","        \"\"\"\n","        Get concatenated outputs from all fingerprint graphs\n","        This creates the input vector for the Univerifier\n","        \"\"\"\n","        outs = []\n","        model.eval()\n","        ctx = torch.enable_grad() if require_grad else torch.no_grad()\n","\n","        with ctx:\n","            for fp in self.fps:\n","                ei = fp.edge_index()\n","\n","                # Create batch tensor for single graph\n","                batch = torch.zeros(fp.n, dtype=torch.long, device=device)\n","\n","                # Get model output (graph-level prediction)\n","                logits = model(fp.X, ei, batch)\n","                probs = F.softmax(logits, dim=-1).flatten()\n","                outs.append(probs)\n","\n","        return torch.cat(outs, dim=0)\n","\n","    def flip_adj_by_grad(self, surrogate_grad_list):\n","        \"\"\"\n","        Apply gradient-based edge flipping to all fingerprints\n","        \"\"\"\n","        for fp, g in zip(self.fps, surrogate_grad_list):\n","            fp.flip_topk_by_grad(g, topk=self.topk_edges, step=self.edge_step)\n","\n"],"metadata":{"id":"zkNJAJL1qHz2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Fingerprint Set Initialization\n","# ================================\n","\"\"\"\n","Initialize the set of graph fingerprints with specified parameters\n","\"\"\"\n","print(\"Initializing graph fingerprint set...\")\n","\n","fp_set = GraphFingerprintSet(\n","    P=CFG[\"FP_P\"],                    # Number of fingerprint graphs\n","    n_nodes=CFG[\"FP_NODES\"],          # Nodes per graph\n","    feat_dim=num_feats,               # Feature dimension\n","    edge_init_p=CFG[\"FP_EDGE_INIT_P\"], # Initial edge probability\n","    topk_edges=CFG[\"FP_EDGE_TOPK\"],   # Top-k edges to flip\n","    edge_step=CFG[\"EDGE_LOGIT_STEP\"],  # Edge update step size\n",")\n","\n","# Calculate Univerifier input dimension\n","# Each fingerprint produces a probability vector of size num_classes\n","INPUT_DIM = CFG[\"FP_P\"] * num_classes\n","print(f\"Fingerprint set initialized:\")\n","print(f\"- {CFG['FP_P']} fingerprint graphs\")\n","print(f\"- {CFG['FP_NODES']} nodes per graph\")\n","print(f\"- Univerifier input dimension: {INPUT_DIM}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tL8LoFEzqNyH","outputId":"56139c21-02f8-4cb3-eaf3-5c04905c9c7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing graph fingerprint set...\n","Fingerprint set initialized:\n","- 64 fingerprint graphs\n","- 32 nodes per graph\n","- Univerifier input dimension: 128\n","\n"]}]},{"cell_type":"code","source":["# ================================\n","# Univerifier Implementation\n","# ================================\n","\"\"\"\n","Univerifier: Binary classifier for ownership verification (Section 3.4.1)\n","Takes concatenated outputs from fingerprints and predicts pirated vs. irrelevant\n","\"\"\"\n","class Univerifier(nn.Module):\n","    \"\"\"\n","    Binary classifier following paper's architecture\n","    3-layer MLP with LeakyReLU activations as specified in Section 4.1.5\n","    \"\"\"\n","    def __init__(self, input_dim: int):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 128),\n","            nn.LeakyReLU(0.01),\n","            nn.Linear(128, 64),\n","            nn.LeakyReLU(0.01),\n","            nn.Linear(64, 32),\n","            nn.LeakyReLU(0.01),\n","            nn.Linear(32, 2),  # Binary classification: pirated vs irrelevant\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","# Initialize Univerifier\n","print(\"Initializing Univerifier...\")\n","V = Univerifier(INPUT_DIM).to(device)\n","opt_V = Adam(V.parameters(), lr=CFG[\"LR_V\"])\n","print(f\"Univerifier initialized with input dimension {INPUT_DIM}\\n\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HegMZhzUqTPX","outputId":"9d4186fe-203c-42d8-f00a-a30de8c737d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing Univerifier...\n","Univerifier initialized with input dimension 128\n","\n"]}]},{"cell_type":"code","source":["# ================================\n","# Joint Learning Setup\n","# ================================\n","\"\"\"\n","Prepare model pools for joint learning (Section 3.4.2)\n","Training pools include target model + positive/negative models\n","\"\"\"\n","# Combine target model with positive training models\n","models_pos_tr = [model_f.to(device)] + [m.to(device) for m in F_pos_tr]\n","models_neg_tr = [m.to(device) for m in F_neg_tr]\n","\n","print(f\"Joint learning setup:\")\n","print(f\"- Positive training models: {len(models_pos_tr)} (including target)\")\n","print(f\"- Negative training models: {len(models_neg_tr)}\")\n","print(f\"- Total training models: {len(models_pos_tr) + len(models_neg_tr)}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJjacUy3qW2a","outputId":"d2162331-57f8-4b0e-9271-58672896f103"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Joint learning setup:\n","- Positive training models: 51 (including target)\n","- Negative training models: 50\n","- Total training models: 101\n","\n"]}]},{"cell_type":"code","source":["# ================================\n","# Joint Learning Helper Functions\n","# ================================\n","\"\"\"\n","Core functions for joint optimization of fingerprints and verifier\n","\"\"\"\n","def batch_from_pool_graph(fp_set, pos_models, neg_models, *, require_grad: bool):\n","    \"\"\"\n","    Create training batch from model pools\n","    Returns: (X, y) where X are concatenated outputs, y are labels\n","    \"\"\"\n","    X = []\n","    y = []\n","\n","    # Positive models (should be classified as pirated)\n","    for m in pos_models:\n","        X.append(fp_set.concat_outputs(m, require_grad=require_grad))\n","        y.append(1)\n","\n","    # Negative models (should be classified as irrelevant)\n","    for m in neg_models:\n","        X.append(fp_set.concat_outputs(m, require_grad=require_grad))\n","        y.append(0)\n","\n","    return torch.stack(X, dim=0), torch.tensor(y, device=device)\n","\n","def surrogate_grad_A_for_graph_fp(fp, model):\n","    \"\"\"\n","    Compute surrogate gradient for adjacency matrix (Section 3.4.2)\n","    Uses node similarity as proxy for actual gradient computation\n","    \"\"\"\n","    with torch.no_grad():\n","        ei = fp.edge_index()\n","        batch = torch.zeros(fp.n, dtype=torch.long, device=device)\n","\n","        # Get intermediate representation from first conv layer\n","        h = model.conv1(fp.X, ei)\n","        h = F.relu(h)\n","\n","        # Compute pairwise node similarity\n","        hn = F.normalize(h, dim=-1)\n","        sim = hn @ hn.t()\n","\n","        # Surrogate gradient: similarity above threshold suggests edge should exist\n","        gradA = sim - 0.5\n","        return gradA.detach().cpu()\n","\n","def update_features_graph(fp_set, V, pos_models, neg_models, steps, lr_x):\n","    \"\"\"\n","    Update fingerprint node features (Section 3.4.2)\n","    Alternates between feature updates and edge updates\n","    \"\"\"\n","    # Freeze all model parameters during feature optimization\n","    for m in pos_models + neg_models:\n","        for p in m.parameters():\n","            p.requires_grad_(False)\n","\n","    # Enable gradients for fingerprint features only\n","    for fp in fp_set.fps:\n","        fp.X.requires_grad_(True)\n","\n","    for _ in range(steps):\n","        # Get training batch with gradients enabled\n","        Xb, yb = batch_from_pool_graph(fp_set, pos_models, neg_models, require_grad=True)\n","\n","        # Freeze verifier during feature update\n","        V.eval()\n","        for p in V.parameters():\n","            p.requires_grad_(False)\n","\n","        # Compute loss for feature optimization\n","        logits = V(Xb.to(device))\n","        loss = F.cross_entropy(logits, yb)\n","\n","        # Zero existing gradients\n","        for fp in fp_set.fps:\n","            if fp.X.grad is not None:\n","                fp.X.grad.zero_()\n","\n","        # Backpropagate to get feature gradients\n","        loss.backward()\n","\n","        # Update features using computed gradients\n","        with torch.no_grad():\n","            for fp in fp_set.fps:\n","                if fp.X.grad is not None:\n","                    fp.X.add_(lr_x * fp.X.grad)\n","                    fp.X.grad.zero_()\n","\n","        # Re-enable verifier gradients\n","        for p in V.parameters():\n","            p.requires_grad_(True)\n","\n","    # Compute surrogate gradients for adjacency matrices\n","    grads = [surrogate_grad_A_for_graph_fp(fp, pos_models[0]) for fp in fp_set.fps]\n","    fp_set.flip_adj_by_grad(grads)\n","\n","def update_verifier_graph(fp_set, V, pos_models, neg_models, steps):\n","    \"\"\"\n","    Update verifier parameters (Section 3.4.2)\n","    Standard supervised learning on model outputs\n","    \"\"\"\n","    for _ in range(steps):\n","        V.train()\n","\n","        # Get training batch (no gradients needed for fingerprints)\n","        Xb, yb = batch_from_pool_graph(fp_set, pos_models, neg_models, require_grad=False)\n","\n","        # Forward pass and loss computation\n","        logits = V(Xb.to(device))\n","        loss = F.cross_entropy(logits, yb)\n","\n","        # Update verifier parameters\n","        opt_V.zero_grad()\n","        loss.backward()\n","        opt_V.step()"],"metadata":{"id":"5U7Wzd_Gqcxx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Joint Learning Loop\n","# ================================\n","\"\"\"\n","Main joint optimization loop (Section 3.4.2)\n","Alternates between fingerprint updates and verifier updates\n","\"\"\"\n","print(\"Starting joint learning...\")\n","print(\"Iter | Overall Acc | Positive Acc | Negative Acc\")\n","print(\"-\" * 50)\n","\n","for it in range(1, CFG[\"OUTER_ITERS\"] + 1):\n","    # Update fingerprint features and adjacency matrices\n","    update_features_graph(\n","        fp_set, V, models_pos_tr, models_neg_tr,\n","        steps=CFG[\"FP_STEPS\"], lr_x=CFG[\"LR_X\"]\n","    )\n","\n","    # Update verifier parameters\n","    update_verifier_graph(\n","        fp_set, V, models_pos_tr, models_neg_tr,\n","        steps=CFG[\"V_STEPS\"]\n","    )\n","\n","    # Evaluate training progress\n","    V.eval()\n","    with torch.no_grad():\n","        Xb, yb = batch_from_pool_graph(fp_set, models_pos_tr, models_neg_tr, require_grad=False)\n","        pred = V(Xb).argmax(dim=1)\n","\n","        # Overall accuracy\n","        acc = (pred.cpu() == yb.cpu()).float().mean().item()\n","\n","        # Positive accuracy (robustness)\n","        pos_acc = (pred[:len(models_pos_tr)].cpu() == 1).float().mean().item()\n","\n","        # Negative accuracy (uniqueness)\n","        neg_acc = (pred[len(models_pos_tr):].cpu() == 0).float().mean().item()\n","\n","    print(f\"{it:4d} | {acc:11.3f} | {pos_acc:12.3f} | {neg_acc:12.3f}\")\n","\n","print(\"\\nJoint learning completed!\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwyJnTmyqfsP","outputId":"1ea59673-3794-462b-d50f-c12714cd127e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting joint learning...\n","Iter | Overall Acc | Positive Acc | Negative Acc\n","--------------------------------------------------\n","   1 |       0.485 |        0.412 |        0.560\n","   2 |       0.475 |        0.353 |        0.600\n","   3 |       0.554 |        0.529 |        0.580\n","   4 |       0.683 |        0.647 |        0.720\n","   5 |       0.752 |        0.745 |        0.760\n","   6 |       0.733 |        0.647 |        0.820\n","   7 |       0.842 |        0.922 |        0.760\n","   8 |       0.832 |        0.882 |        0.780\n","   9 |       0.842 |        0.922 |        0.760\n","  10 |       0.842 |        0.922 |        0.760\n","  11 |       0.832 |        0.882 |        0.780\n","  12 |       0.832 |        0.902 |        0.760\n","  13 |       0.832 |        0.922 |        0.740\n","  14 |       0.822 |        0.902 |        0.740\n","  15 |       0.851 |        0.902 |        0.800\n","  16 |       0.851 |        0.902 |        0.800\n","  17 |       0.851 |        0.922 |        0.780\n","  18 |       0.842 |        0.922 |        0.760\n","  19 |       0.881 |        0.922 |        0.840\n","  20 |       0.842 |        0.922 |        0.760\n","\n","Joint learning completed!\n","\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Test Set Evaluation\n","# ================================\n","\"\"\"\n","Evaluate on held-out test sets to measure final performance\n","Following paper's evaluation methodology (Section 4.1.4)\n","\"\"\"\n","print(\"Evaluating on test sets...\")\n","\n","# Prepare test model pools\n","models_pos_te = [model_f.to(device)] + [m.to(device) for m in F_pos_te]\n","models_neg_te = [m.to(device) for m in F_neg_te]\n","\n","@torch.no_grad()\n","def verify_scores_graph(V, fp_set, models):\n","    \"\"\"\n","    Get verification scores (probability of being pirated) for models\n","    \"\"\"\n","    V.eval()\n","    Xs = [fp_set.concat_outputs(m, require_grad=False) for m in models]\n","    X_batch = torch.stack(Xs, dim=0).to(device)\n","    logits = V(X_batch)\n","    probs = F.softmax(logits, dim=-1)[:, 1]  # Probability of positive class\n","    return probs.detach().cpu().numpy()\n","\n","# Get verification scores for test models\n","p_pos = verify_scores_graph(V, fp_set, models_pos_te)  # Positive model scores\n","p_neg = verify_scores_graph(V, fp_set, models_neg_te)  # Negative model scores\n","\n","print(f\"Test set sizes: {len(models_pos_te)} positive, {len(models_neg_te)} negative\")\n","print(f\"Sample positive scores: {p_pos[:5]}\")\n","print(f\"Sample negative scores: {p_neg[:5]}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FWdwz52YqnlA","outputId":"369ea11b-42e1-4433-ab37-6c545e57ee80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating on test sets...\n","Test set sizes: 51 positive, 50 negative\n","Sample positive scores: [0.7363378  0.3280269  0.93478674 0.8450202  0.30074674]\n","Sample negative scores: [0.03424155 0.06295612 0.0069764  0.01874819 0.0108477 ]\n","\n"]}]},{"cell_type":"code","source":["# ================================\n","#  Threshold Sweep & Metrics\n","# ================================\n","\"\"\"\n","Compute robustness, uniqueness, and ARUC across threshold values\n","Following paper's evaluation metrics (Section 4.1.4)\n","\"\"\"\n","def sweep_threshold_graph(p_pos, p_neg, num=301):\n","    \"\"\"\n","    Sweep threshold values to compute robustness and uniqueness curves\n","    \"\"\"\n","    ths = np.linspace(0.0, 1.0, num=num)\n","    R = []  # Robustness (True Positive Rate)\n","    U = []  # Uniqueness (True Negative Rate)\n","    A = []  # Balanced Accuracy\n","\n","    for t in ths:\n","        tp = (p_pos >= t).mean()    # Robustness: correctly identify positive\n","        tn = (p_neg <  t).mean()    # Uniqueness: correctly identify negative\n","        R.append(tp)\n","        U.append(tn)\n","        A.append((tp + tn) / 2.0)   # Balanced accuracy\n","\n","    return ths, np.array(R), np.array(U), np.array(A)\n","\n","# Perform threshold sweep\n","ths, R, U, A = sweep_threshold_graph(p_pos, p_neg, num=301)\n","\n","# Find best threshold and metrics\n","best_idx = A.argmax()\n","mean_acc = A.mean()\n","\n","# Calculate ARUC (Area under Robustness-Uniqueness Curve)\n","try:\n","    # For numpy >= 2.0\n","    ARUC = np.trapezoid(np.minimum(R, U), ths)\n","except AttributeError:\n","    # For numpy < 2.0\n","    ARUC = np.trapz(np.minimum(R, U), ths)\n","\n","# ================================\n","\"\"\"\n","Display final results and compare with paper benchmarks\n","\"\"\"\n","print(\"=\" * 60)\n","print(\"FINAL RESULTS - PROTEINS Graph Classification\")\n","print(\"=\" * 60)\n","print(f\"Best threshold λ = {ths[best_idx]:.3f}\")\n","print(f\"Robustness (True Positive Rate) = {R[best_idx]:.3f}\")\n","print(f\"Uniqueness (True Negative Rate) = {U[best_idx]:.3f}\")\n","print(f\"Mean Test Accuracy = {A[best_idx]:.3f}\")\n","print(f\"Average Test Accuracy (over all λ) = {mean_acc:.3f}\")\n","print(f\"ARUC (Area Under RU Curve) = {ARUC:.3f}\")\n","print(\"=\" * 60)\n","\n","# Compare with paper results\n","print(\"\\nComparison with Paper Results (Table 1 - PROTEINS):\")\n","print(\"Paper Benchmarks:\")\n","print(\"- GCNMean:      0.967\")\n","print(\"- GCNDiff:      0.961\")\n","print(\"- GraphsageMean: 0.989\")\n","print(\"- GraphsageDiff: 0.984\")\n","print(f\"\\nYour Implementation:\")\n","print(f\"- Mean Accuracy: {A[best_idx]:.3f}\")\n","print(f\"- ARUC Score:    {ARUC:.3f}\")\n","\n","# Performance assessment\n","if A[best_idx] >= 0.96:\n","    print(\"\\n✅ EXCELLENT! Results match/exceed paper performance!\")\n","elif A[best_idx] >= 0.90:\n","    print(\"\\n✅ VERY GOOD! Results are close to paper performance!\")\n","elif A[best_idx] >= 0.80:\n","    print(\"\\n⚠️  DECENT! Results are reasonable but could be improved.\")\n","    print(\"   Consider: increasing joint learning iterations, tuning hyperparameters\")\n","else:\n","    print(\"\\n❌ NEEDS IMPROVEMENT! Check implementation details.\")\n","    print(\"   Focus on: gradient computation, edge flipping logic, feature updates\")\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"GNNFingers Implementation Complete!\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2auTLpQAqtTS","outputId":"f765fb1b-1358-458e-8a55-cedf1a6c781a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","FINAL RESULTS - PROTEINS Graph Classification\n","============================================================\n","Best threshold λ = 0.793\n","Robustness (True Positive Rate) = 0.765\n","Uniqueness (True Negative Rate) = 0.780\n","Mean Test Accuracy = 0.772\n","Average Test Accuracy (over all λ) = 0.713\n","ARUC (Area Under RU Curve) = 0.547\n","============================================================\n","\n","Comparison with Paper Results (Table 1 - PROTEINS):\n","Paper Benchmarks:\n","- GCNMean:      0.967\n","- GCNDiff:      0.961\n","- GraphsageMean: 0.989\n","- GraphsageDiff: 0.984\n","\n","Your Implementation:\n","- Mean Accuracy: 0.772\n","- ARUC Score:    0.547\n","\n","❌ NEEDS IMPROVEMENT! Check implementation details.\n","   Focus on: gradient computation, edge flipping logic, feature updates\n","\n","============================================================\n","GNNFingers Implementation Complete!\n","============================================================\n"]}]}]}